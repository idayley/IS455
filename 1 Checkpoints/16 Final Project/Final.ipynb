{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection and Retrieval\n",
    "Data Collection and Retrieval (same requirements as the individual data retrieval project):\n",
    "* Topic: anything you choose, but it must include the following data types:\n",
    "    * numeric features\n",
    "    * categorical features (to be dummy coded)\n",
    "    * text features (to be processed using text analytics)\n",
    "    * image features (to be processed using image classification) \n",
    "    * labels to choose from (i.e. outcomes that you want to predict with the other features)\n",
    "* Scrape approximately ~500 records; +/- 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyLDAvis\n",
    "# !pip install pyLDAvis.gensim\n",
    "# !pip install logging\n",
    "# !pip install nltk\n",
    "# !pip install - U pip setuptools wheel\n",
    "# !pip install - U spacy\n",
    "# !python - m spacy download en_core_web_sm\n",
    "# !pip install gower\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def get_tweets(bearer_token):\n",
    "    headers = {'Authorization': ('Bearer ' + bearer_token)}\n",
    "\n",
    "    n = 525                               # The total number of tweets we want\n",
    "    max_results = 10 # The number of tweets to pull per request; must be between 10 and 100\n",
    "    total_retrieved = 0                   # To keep track of when to stop\n",
    "    next_token = \"\"                       # Must be empty on first iteration\n",
    "    search_term = \"manchester%20united\"\n",
    "\n",
    "    # Create empty DataFrames and set columns\n",
    "    df_tweets = pd.DataFrame(columns=['tweet_id', 'author_id', 'retweet_count', 'like_count',\n",
    "                                      'text', 'language', 'created_at', 'source', 'possibly_sensitive', 'image_url'])\n",
    "\n",
    "    images_retrieved = 0\n",
    "\n",
    "    # stop when we have n results\n",
    "    while images_retrieved < n:\n",
    "\n",
    "        # the first time through the loop, we do not need the next_token parameter\n",
    "        if next_token == \"\":\n",
    "            url = f'https://api.twitter.com/2/tweets/search/recent?query={search_term}&max_results={max_results}'\n",
    "        else:\n",
    "            url = f'https://api.twitter.com/2/tweets/search/recent?query={search_term}&max_results={max_results}&next_token={next_token}'\n",
    "\n",
    "        # These are the extra parameters we will add to the querystring; we won't store them all though; just want you to see what's possible\n",
    "        url += f'&expansions=geo.place_id,author_id,attachments.media_keys'\n",
    "        url += f'&tweet.fields=attachments,author_id,context_annotations,conversation_id,created_at,entities,geo,id,in_reply_to_user_id,lang,possibly_sensitive,public_metrics,referenced_tweets,reply_settings,source,text,withheld'\n",
    "        url += f'&media.fields=media_key,type,url&user.fields=created_at,description,entities,id,location,name,pinned_tweet_id,profile_image_url,protected,public_metrics,url,username,verified,withheld'\n",
    "\n",
    "        # make the request to the Twitter API Recent Search endpoint\n",
    "        response = requests.request(\"GET\", url, headers=headers)\n",
    "        try:  # Just in case we get an error\n",
    "            json_data = json.loads(response.text)\n",
    "            # print(json_data)\n",
    "        except:\n",
    "            print(response.text)\n",
    "\n",
    "        for tweet in json_data['data']:\n",
    "            media_key = \"\"  # Reset to empty each time through the loop so that we can use it for a condition later\n",
    "\n",
    "            # Store the data into variables\n",
    "            tweet_id = tweet['id']\n",
    "            author_id = tweet['author_id']\n",
    "            retweet_count = tweet['public_metrics']['retweet_count']    # label\n",
    "            like_count = tweet['public_metrics']['like_count']          # label\n",
    "            image_url = \"\"                                              # image\n",
    "            text = tweet['text']                                        # text\n",
    "            created_at = tweet['created_at']                            # categorical\n",
    "            source = tweet['source']                                    # categorical\n",
    "            possibly_sensitive = tweet['possibly_sensitive']            # categorical\n",
    "            language = tweet['lang']                                    # categorical\n",
    "\n",
    "            # Find out if there is media\n",
    "            if 'attachments' in tweet:\n",
    "                if 'media_keys' in tweet['attachments']:\n",
    "                    media_key = tweet['attachments']['media_keys'][0]\n",
    "\n",
    "            # If there is a media key in this tweet, iterate through tweet['includes']['media'] until we find it\n",
    "            if media_key != \"\":\n",
    "                for media in json_data['includes']['media']:\n",
    "                    # Only if the media_key matches the one we stored\n",
    "                    if media['media_key'] == media_key:\n",
    "                        if media['type'] == 'photo':      # Only if it is a photo; ignore videos\n",
    "                            # Store the url in a variable\n",
    "                            image_url = media['url']\n",
    "                            images_retrieved = images_retrieved + 1\n",
    "                            print(\"Images: \" + str(images_retrieved))\n",
    "\n",
    "\n",
    "\n",
    "            # Add the new data to a new record in the DataFrame\n",
    "            df_tweets.loc[tweet_id] = [tweet_id, author_id, retweet_count, like_count,\n",
    "                                       text, language, created_at, source, possibly_sensitive, image_url]\n",
    "\n",
    "        # keep track of how many results have been obtained so far:\n",
    "        total_retrieved += 10\n",
    "        print(f'{total_retrieved} tweets retrieved')\n",
    "\n",
    "        # keep track of where to start next time, but quit if there are no more results\n",
    "        try:\n",
    "            next_token = json_data['meta']['next_token']\n",
    "        except:\n",
    "            break\n",
    "\n",
    "        # sleep to avoid hitting the rate limit\n",
    "        time.sleep(8)\n",
    "\n",
    "    # All done! save the dataframes to csv files\n",
    "    df_tweets.to_csv('tweets.csv')\n",
    "    print('Got all the tweets!')\n",
    "    return df_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images: 1\n",
      "Images: 2\n",
      "Images: 3\n",
      "10 tweets retrieved\n",
      "Images: 4\n",
      "20 tweets retrieved\n",
      "30 tweets retrieved\n",
      "40 tweets retrieved\n",
      "Images: 5\n",
      "50 tweets retrieved\n",
      "Images: 6\n",
      "60 tweets retrieved\n",
      "Images: 7\n",
      "70 tweets retrieved\n",
      "Images: 8\n",
      "80 tweets retrieved\n",
      "Images: 9\n",
      "Images: 10\n",
      "Images: 11\n",
      "90 tweets retrieved\n",
      "Images: 12\n",
      "100 tweets retrieved\n",
      "Images: 13\n",
      "Images: 14\n",
      "Images: 15\n",
      "Images: 16\n",
      "Images: 17\n",
      "110 tweets retrieved\n",
      "Images: 18\n",
      "Images: 19\n",
      "Images: 20\n",
      "Images: 21\n",
      "120 tweets retrieved\n",
      "Images: 22\n",
      "Images: 23\n",
      "130 tweets retrieved\n",
      "140 tweets retrieved\n",
      "150 tweets retrieved\n",
      "Images: 24\n",
      "Images: 25\n",
      "160 tweets retrieved\n",
      "Images: 26\n",
      "Images: 27\n",
      "170 tweets retrieved\n",
      "Images: 28\n",
      "180 tweets retrieved\n",
      "Images: 29\n",
      "Images: 30\n",
      "190 tweets retrieved\n",
      "Images: 31\n",
      "200 tweets retrieved\n",
      "210 tweets retrieved\n",
      "Images: 32\n",
      "Images: 33\n",
      "220 tweets retrieved\n",
      "Images: 34\n",
      "Images: 35\n",
      "Images: 36\n",
      "230 tweets retrieved\n",
      "Images: 37\n",
      "240 tweets retrieved\n",
      "Images: 38\n",
      "250 tweets retrieved\n",
      "Images: 39\n",
      "Images: 40\n",
      "260 tweets retrieved\n",
      "270 tweets retrieved\n",
      "Images: 41\n",
      "280 tweets retrieved\n",
      "Images: 42\n",
      "290 tweets retrieved\n",
      "300 tweets retrieved\n",
      "Images: 43\n",
      "310 tweets retrieved\n",
      "Images: 44\n",
      "320 tweets retrieved\n",
      "Images: 45\n",
      "Images: 46\n",
      "330 tweets retrieved\n",
      "Images: 47\n",
      "Images: 48\n",
      "340 tweets retrieved\n",
      "350 tweets retrieved\n",
      "Images: 49\n",
      "360 tweets retrieved\n",
      "Images: 50\n",
      "370 tweets retrieved\n",
      "Images: 51\n",
      "Images: 52\n",
      "Images: 53\n",
      "380 tweets retrieved\n",
      "390 tweets retrieved\n",
      "Images: 54\n",
      "Images: 55\n",
      "Images: 56\n",
      "Images: 57\n",
      "400 tweets retrieved\n",
      "Images: 58\n",
      "Images: 59\n",
      "Images: 60\n",
      "410 tweets retrieved\n",
      "Images: 61\n",
      "Images: 62\n",
      "Images: 63\n",
      "Images: 64\n",
      "420 tweets retrieved\n",
      "Images: 65\n",
      "Images: 66\n",
      "430 tweets retrieved\n",
      "Images: 67\n",
      "440 tweets retrieved\n",
      "Images: 68\n",
      "450 tweets retrieved\n",
      "Images: 69\n",
      "Images: 70\n",
      "Images: 71\n",
      "460 tweets retrieved\n",
      "470 tweets retrieved\n",
      "Images: 72\n",
      "480 tweets retrieved\n",
      "Images: 73\n",
      "Images: 74\n",
      "490 tweets retrieved\n",
      "Images: 75\n",
      "500 tweets retrieved\n",
      "Images: 76\n",
      "Images: 77\n",
      "Images: 78\n",
      "510 tweets retrieved\n",
      "Images: 79\n",
      "Images: 80\n",
      "Images: 81\n",
      "Images: 82\n",
      "Images: 83\n",
      "520 tweets retrieved\n",
      "530 tweets retrieved\n",
      "540 tweets retrieved\n",
      "Images: 84\n",
      "550 tweets retrieved\n",
      "Images: 85\n",
      "Images: 86\n",
      "560 tweets retrieved\n",
      "Images: 87\n",
      "Images: 88\n",
      "570 tweets retrieved\n",
      "580 tweets retrieved\n",
      "Images: 89\n",
      "Images: 90\n",
      "Images: 91\n",
      "Images: 92\n",
      "590 tweets retrieved\n",
      "Images: 93\n",
      "Images: 94\n",
      "600 tweets retrieved\n",
      "Images: 95\n",
      "Images: 96\n",
      "610 tweets retrieved\n",
      "Images: 97\n",
      "620 tweets retrieved\n",
      "630 tweets retrieved\n",
      "Images: 98\n",
      "Images: 99\n",
      "640 tweets retrieved\n",
      "Images: 100\n",
      "Images: 101\n",
      "650 tweets retrieved\n",
      "Images: 102\n",
      "Images: 103\n",
      "Images: 104\n",
      "Images: 105\n",
      "660 tweets retrieved\n",
      "Images: 106\n",
      "670 tweets retrieved\n",
      "680 tweets retrieved\n",
      "Images: 107\n",
      "Images: 108\n",
      "690 tweets retrieved\n",
      "Images: 109\n",
      "Images: 110\n",
      "700 tweets retrieved\n",
      "Images: 111\n",
      "Images: 112\n",
      "Images: 113\n",
      "Images: 114\n",
      "710 tweets retrieved\n",
      "Images: 115\n",
      "Images: 116\n",
      "720 tweets retrieved\n",
      "730 tweets retrieved\n",
      "Images: 117\n",
      "Images: 118\n",
      "740 tweets retrieved\n",
      "750 tweets retrieved\n",
      "Images: 119\n",
      "760 tweets retrieved\n",
      "Images: 120\n",
      "Images: 121\n",
      "770 tweets retrieved\n",
      "Images: 122\n",
      "Images: 123\n",
      "Images: 124\n",
      "780 tweets retrieved\n",
      "Images: 125\n",
      "Images: 126\n",
      "Images: 127\n",
      "790 tweets retrieved\n",
      "Images: 128\n",
      "Images: 129\n",
      "800 tweets retrieved\n",
      "Images: 130\n",
      "Images: 131\n",
      "Images: 132\n",
      "810 tweets retrieved\n",
      "Images: 133\n",
      "Images: 134\n",
      "820 tweets retrieved\n",
      "Images: 135\n",
      "Images: 136\n",
      "Images: 137\n",
      "830 tweets retrieved\n",
      "Images: 138\n",
      "Images: 139\n",
      "Images: 140\n",
      "840 tweets retrieved\n",
      "Images: 141\n",
      "Images: 142\n",
      "850 tweets retrieved\n",
      "Images: 143\n",
      "Images: 144\n",
      "Images: 145\n",
      "860 tweets retrieved\n",
      "Images: 146\n",
      "870 tweets retrieved\n",
      "Images: 147\n",
      "880 tweets retrieved\n",
      "Images: 148\n",
      "890 tweets retrieved\n",
      "Images: 149\n",
      "Images: 150\n",
      "Images: 151\n",
      "Images: 152\n",
      "900 tweets retrieved\n",
      "Images: 153\n",
      "910 tweets retrieved\n",
      "Images: 154\n",
      "Images: 155\n",
      "Images: 156\n",
      "920 tweets retrieved\n",
      "Images: 157\n",
      "Images: 158\n",
      "930 tweets retrieved\n",
      "940 tweets retrieved\n",
      "Images: 159\n",
      "Images: 160\n",
      "950 tweets retrieved\n",
      "Images: 161\n",
      "Images: 162\n",
      "Images: 163\n",
      "960 tweets retrieved\n",
      "Images: 164\n",
      "Images: 165\n",
      "Images: 166\n",
      "Images: 167\n",
      "970 tweets retrieved\n",
      "Images: 168\n",
      "Images: 169\n",
      "Images: 170\n",
      "Images: 171\n",
      "980 tweets retrieved\n",
      "Images: 172\n",
      "990 tweets retrieved\n",
      "Images: 173\n",
      "Images: 174\n",
      "Images: 175\n",
      "Images: 176\n",
      "1000 tweets retrieved\n",
      "Images: 177\n",
      "Images: 178\n",
      "Images: 179\n",
      "1010 tweets retrieved\n",
      "Images: 180\n",
      "Images: 181\n",
      "1020 tweets retrieved\n",
      "Images: 182\n",
      "Images: 183\n",
      "Images: 184\n",
      "1030 tweets retrieved\n",
      "Images: 185\n",
      "Images: 186\n",
      "Images: 187\n",
      "Images: 188\n",
      "Images: 189\n",
      "Images: 190\n",
      "Images: 191\n",
      "1040 tweets retrieved\n",
      "Images: 192\n",
      "1050 tweets retrieved\n",
      "Images: 193\n",
      "Images: 194\n",
      "1060 tweets retrieved\n",
      "Images: 195\n",
      "Images: 196\n",
      "Images: 197\n",
      "1070 tweets retrieved\n",
      "Images: 198\n",
      "Images: 199\n",
      "Images: 200\n",
      "1080 tweets retrieved\n",
      "Images: 201\n",
      "1090 tweets retrieved\n",
      "Images: 202\n",
      "Images: 203\n",
      "Images: 204\n",
      "1100 tweets retrieved\n",
      "Images: 205\n",
      "Images: 206\n",
      "1110 tweets retrieved\n",
      "Images: 207\n",
      "Images: 208\n",
      "1120 tweets retrieved\n",
      "Images: 209\n",
      "Images: 210\n",
      "1130 tweets retrieved\n",
      "1140 tweets retrieved\n",
      "Images: 211\n",
      "1150 tweets retrieved\n",
      "Images: 212\n",
      "1160 tweets retrieved\n",
      "Images: 213\n",
      "Images: 214\n",
      "1170 tweets retrieved\n",
      "Images: 215\n",
      "1180 tweets retrieved\n",
      "1190 tweets retrieved\n",
      "Images: 216\n",
      "Images: 217\n",
      "1200 tweets retrieved\n",
      "Images: 218\n",
      "Images: 219\n",
      "Images: 220\n",
      "1210 tweets retrieved\n",
      "Images: 221\n",
      "Images: 222\n",
      "1220 tweets retrieved\n",
      "Images: 223\n",
      "Images: 224\n",
      "1230 tweets retrieved\n",
      "Images: 225\n",
      "1240 tweets retrieved\n",
      "1250 tweets retrieved\n",
      "1260 tweets retrieved\n",
      "1270 tweets retrieved\n",
      "1280 tweets retrieved\n",
      "1290 tweets retrieved\n",
      "Images: 226\n",
      "Images: 227\n",
      "1300 tweets retrieved\n",
      "Images: 228\n",
      "Images: 229\n",
      "1310 tweets retrieved\n",
      "Images: 230\n",
      "1320 tweets retrieved\n",
      "1330 tweets retrieved\n",
      "1340 tweets retrieved\n",
      "1350 tweets retrieved\n",
      "Images: 231\n",
      "Images: 232\n",
      "Images: 233\n",
      "Images: 234\n",
      "1360 tweets retrieved\n",
      "Images: 235\n",
      "Images: 236\n",
      "Images: 237\n",
      "1370 tweets retrieved\n",
      "1380 tweets retrieved\n",
      "1390 tweets retrieved\n",
      "1400 tweets retrieved\n",
      "1410 tweets retrieved\n",
      "Images: 238\n",
      "1420 tweets retrieved\n",
      "1430 tweets retrieved\n",
      "1440 tweets retrieved\n",
      "Images: 239\n",
      "Images: 240\n",
      "1450 tweets retrieved\n",
      "Images: 241\n",
      "1460 tweets retrieved\n",
      "Images: 242\n",
      "Images: 243\n",
      "1470 tweets retrieved\n",
      "Images: 244\n",
      "1480 tweets retrieved\n",
      "1490 tweets retrieved\n",
      "Images: 245\n",
      "Images: 246\n",
      "Images: 247\n",
      "1500 tweets retrieved\n",
      "Images: 248\n",
      "1510 tweets retrieved\n",
      "Images: 249\n",
      "1520 tweets retrieved\n",
      "Images: 250\n",
      "Images: 251\n",
      "1530 tweets retrieved\n",
      "Images: 252\n",
      "Images: 253\n",
      "1540 tweets retrieved\n",
      "Images: 254\n",
      "1550 tweets retrieved\n",
      "Images: 255\n",
      "1560 tweets retrieved\n",
      "1570 tweets retrieved\n",
      "1580 tweets retrieved\n",
      "Images: 256\n",
      "1590 tweets retrieved\n",
      "1600 tweets retrieved\n",
      "1610 tweets retrieved\n",
      "Images: 257\n",
      "Images: 258\n",
      "1620 tweets retrieved\n",
      "1630 tweets retrieved\n",
      "Images: 259\n",
      "1640 tweets retrieved\n",
      "Images: 260\n",
      "1650 tweets retrieved\n",
      "Images: 261\n",
      "1660 tweets retrieved\n",
      "Images: 262\n",
      "1670 tweets retrieved\n",
      "Images: 263\n",
      "1680 tweets retrieved\n",
      "1690 tweets retrieved\n",
      "Images: 264\n",
      "1700 tweets retrieved\n",
      "Images: 265\n",
      "1710 tweets retrieved\n",
      "Images: 266\n",
      "1720 tweets retrieved\n",
      "Images: 267\n",
      "Images: 268\n",
      "Images: 269\n",
      "1730 tweets retrieved\n",
      "1740 tweets retrieved\n",
      "Images: 270\n",
      "1750 tweets retrieved\n",
      "1760 tweets retrieved\n",
      "Images: 271\n",
      "1770 tweets retrieved\n",
      "1780 tweets retrieved\n",
      "1790 tweets retrieved\n",
      "Images: 272\n",
      "Images: 273\n",
      "1800 tweets retrieved\n",
      "1810 tweets retrieved\n",
      "1820 tweets retrieved\n",
      "Images: 274\n",
      "1830 tweets retrieved\n",
      "Images: 275\n",
      "1840 tweets retrieved\n",
      "Images: 276\n",
      "1850 tweets retrieved\n",
      "Images: 277\n",
      "Images: 278\n",
      "Images: 279\n",
      "1860 tweets retrieved\n",
      "Images: 280\n",
      "1870 tweets retrieved\n",
      "Images: 281\n",
      "Images: 282\n",
      "1880 tweets retrieved\n",
      "Images: 283\n",
      "Images: 284\n",
      "Images: 285\n",
      "1890 tweets retrieved\n",
      "Images: 286\n",
      "Images: 287\n",
      "1900 tweets retrieved\n",
      "Images: 288\n",
      "1910 tweets retrieved\n",
      "1920 tweets retrieved\n",
      "Images: 289\n",
      "1930 tweets retrieved\n",
      "Images: 290\n",
      "1940 tweets retrieved\n",
      "Images: 291\n",
      "Images: 292\n",
      "1950 tweets retrieved\n",
      "Images: 293\n",
      "1960 tweets retrieved\n",
      "Images: 294\n",
      "1970 tweets retrieved\n",
      "Images: 295\n",
      "1980 tweets retrieved\n",
      "Images: 296\n",
      "1990 tweets retrieved\n",
      "2000 tweets retrieved\n",
      "Images: 297\n",
      "2010 tweets retrieved\n",
      "Images: 298\n",
      "Images: 299\n",
      "Images: 300\n",
      "2020 tweets retrieved\n",
      "2030 tweets retrieved\n",
      "Images: 301\n",
      "Images: 302\n",
      "2040 tweets retrieved\n",
      "Images: 303\n",
      "2050 tweets retrieved\n",
      "Images: 304\n",
      "2060 tweets retrieved\n",
      "2070 tweets retrieved\n",
      "Images: 305\n",
      "2080 tweets retrieved\n",
      "Images: 306\n",
      "2090 tweets retrieved\n",
      "2100 tweets retrieved\n",
      "Images: 307\n",
      "2110 tweets retrieved\n",
      "2120 tweets retrieved\n",
      "Images: 308\n",
      "2130 tweets retrieved\n",
      "Images: 309\n",
      "2140 tweets retrieved\n",
      "2150 tweets retrieved\n",
      "Images: 310\n",
      "2160 tweets retrieved\n",
      "Images: 311\n",
      "Images: 312\n",
      "2170 tweets retrieved\n",
      "2180 tweets retrieved\n",
      "2190 tweets retrieved\n",
      "2200 tweets retrieved\n",
      "2210 tweets retrieved\n",
      "Images: 313\n",
      "2220 tweets retrieved\n",
      "2230 tweets retrieved\n",
      "2240 tweets retrieved\n",
      "Images: 314\n",
      "2250 tweets retrieved\n",
      "Images: 315\n",
      "2260 tweets retrieved\n",
      "Images: 316\n",
      "2270 tweets retrieved\n",
      "2280 tweets retrieved\n",
      "Images: 317\n",
      "Images: 318\n",
      "2290 tweets retrieved\n",
      "Images: 319\n",
      "2300 tweets retrieved\n",
      "2310 tweets retrieved\n",
      "Images: 320\n",
      "Images: 321\n",
      "2320 tweets retrieved\n",
      "2330 tweets retrieved\n",
      "Images: 322\n",
      "2340 tweets retrieved\n",
      "Images: 323\n",
      "Images: 324\n",
      "Images: 325\n",
      "2350 tweets retrieved\n",
      "2360 tweets retrieved\n",
      "2370 tweets retrieved\n",
      "2380 tweets retrieved\n",
      "Images: 326\n",
      "2390 tweets retrieved\n",
      "2400 tweets retrieved\n",
      "Images: 327\n",
      "Images: 328\n",
      "2410 tweets retrieved\n",
      "Images: 329\n",
      "2420 tweets retrieved\n",
      "2430 tweets retrieved\n",
      "2440 tweets retrieved\n",
      "Images: 330\n",
      "2450 tweets retrieved\n",
      "Images: 331\n",
      "Images: 332\n",
      "2460 tweets retrieved\n",
      "Images: 333\n",
      "2470 tweets retrieved\n",
      "Images: 334\n",
      "2480 tweets retrieved\n",
      "Images: 335\n",
      "Images: 336\n",
      "Images: 337\n",
      "2490 tweets retrieved\n",
      "Images: 338\n",
      "2500 tweets retrieved\n",
      "Images: 339\n",
      "2510 tweets retrieved\n",
      "2520 tweets retrieved\n",
      "Images: 340\n",
      "Images: 341\n",
      "2530 tweets retrieved\n",
      "Images: 342\n",
      "Images: 343\n",
      "2540 tweets retrieved\n",
      "Images: 344\n",
      "Images: 345\n",
      "2550 tweets retrieved\n",
      "Images: 346\n",
      "Images: 347\n",
      "2560 tweets retrieved\n",
      "2570 tweets retrieved\n",
      "Images: 348\n",
      "2580 tweets retrieved\n",
      "2590 tweets retrieved\n",
      "2600 tweets retrieved\n",
      "Images: 349\n",
      "2610 tweets retrieved\n",
      "2620 tweets retrieved\n",
      "2630 tweets retrieved\n",
      "Images: 350\n",
      "Images: 351\n",
      "2640 tweets retrieved\n",
      "Images: 352\n",
      "2650 tweets retrieved\n",
      "2660 tweets retrieved\n",
      "Images: 353\n",
      "2670 tweets retrieved\n",
      "Images: 354\n",
      "Images: 355\n",
      "2680 tweets retrieved\n",
      "2690 tweets retrieved\n",
      "2700 tweets retrieved\n",
      "2710 tweets retrieved\n",
      "Images: 356\n",
      "2720 tweets retrieved\n",
      "Images: 357\n",
      "2730 tweets retrieved\n",
      "Images: 358\n",
      "Images: 359\n",
      "Images: 360\n",
      "2740 tweets retrieved\n",
      "2750 tweets retrieved\n",
      "Images: 361\n",
      "2760 tweets retrieved\n",
      "2770 tweets retrieved\n",
      "2780 tweets retrieved\n",
      "Images: 362\n",
      "2790 tweets retrieved\n",
      "2800 tweets retrieved\n",
      "2810 tweets retrieved\n",
      "2820 tweets retrieved\n",
      "Images: 363\n",
      "2830 tweets retrieved\n",
      "2840 tweets retrieved\n",
      "2850 tweets retrieved\n",
      "Images: 364\n",
      "2860 tweets retrieved\n",
      "Images: 365\n",
      "2870 tweets retrieved\n",
      "Images: 366\n",
      "2880 tweets retrieved\n",
      "Images: 367\n",
      "2890 tweets retrieved\n",
      "2900 tweets retrieved\n",
      "2910 tweets retrieved\n",
      "Images: 368\n",
      "Images: 369\n",
      "2920 tweets retrieved\n",
      "Images: 370\n",
      "2930 tweets retrieved\n",
      "Images: 371\n",
      "2940 tweets retrieved\n",
      "Images: 372\n",
      "2950 tweets retrieved\n",
      "2960 tweets retrieved\n",
      "2970 tweets retrieved\n",
      "Images: 373\n",
      "2980 tweets retrieved\n",
      "2990 tweets retrieved\n",
      "3000 tweets retrieved\n",
      "Images: 374\n",
      "3010 tweets retrieved\n",
      "3020 tweets retrieved\n",
      "3030 tweets retrieved\n",
      "3040 tweets retrieved\n",
      "3050 tweets retrieved\n",
      "3060 tweets retrieved\n",
      "Images: 375\n",
      "3070 tweets retrieved\n",
      "3080 tweets retrieved\n",
      "3090 tweets retrieved\n",
      "Images: 376\n",
      "Images: 377\n",
      "3100 tweets retrieved\n",
      "3110 tweets retrieved\n",
      "Images: 378\n",
      "3120 tweets retrieved\n",
      "3130 tweets retrieved\n",
      "3140 tweets retrieved\n",
      "Images: 379\n",
      "3150 tweets retrieved\n",
      "3160 tweets retrieved\n",
      "Images: 380\n",
      "3170 tweets retrieved\n",
      "3180 tweets retrieved\n",
      "3190 tweets retrieved\n",
      "Images: 381\n",
      "3200 tweets retrieved\n",
      "3210 tweets retrieved\n",
      "3220 tweets retrieved\n",
      "3230 tweets retrieved\n",
      "3240 tweets retrieved\n",
      "3250 tweets retrieved\n",
      "Images: 382\n",
      "3260 tweets retrieved\n",
      "Images: 383\n",
      "3270 tweets retrieved\n",
      "Images: 384\n",
      "Images: 385\n",
      "3280 tweets retrieved\n",
      "Images: 386\n",
      "3290 tweets retrieved\n",
      "3300 tweets retrieved\n",
      "Images: 387\n",
      "3310 tweets retrieved\n",
      "3320 tweets retrieved\n",
      "Images: 388\n",
      "3330 tweets retrieved\n",
      "Images: 389\n",
      "3340 tweets retrieved\n",
      "Images: 390\n",
      "3350 tweets retrieved\n",
      "Images: 391\n",
      "3360 tweets retrieved\n",
      "3370 tweets retrieved\n",
      "Images: 392\n",
      "3380 tweets retrieved\n",
      "3390 tweets retrieved\n",
      "Images: 393\n",
      "3400 tweets retrieved\n",
      "Images: 394\n",
      "3410 tweets retrieved\n",
      "3420 tweets retrieved\n",
      "Images: 395\n",
      "Images: 396\n",
      "3430 tweets retrieved\n",
      "Images: 397\n",
      "Images: 398\n",
      "3440 tweets retrieved\n",
      "Images: 399\n",
      "3450 tweets retrieved\n",
      "3460 tweets retrieved\n",
      "3470 tweets retrieved\n",
      "Images: 400\n",
      "Images: 401\n",
      "3480 tweets retrieved\n",
      "Images: 402\n",
      "3490 tweets retrieved\n",
      "Images: 403\n",
      "Images: 404\n",
      "3500 tweets retrieved\n",
      "3510 tweets retrieved\n",
      "Images: 405\n",
      "3520 tweets retrieved\n",
      "3530 tweets retrieved\n",
      "3540 tweets retrieved\n",
      "3550 tweets retrieved\n",
      "3560 tweets retrieved\n",
      "3570 tweets retrieved\n",
      "Images: 406\n",
      "3580 tweets retrieved\n",
      "3590 tweets retrieved\n",
      "3600 tweets retrieved\n",
      "3610 tweets retrieved\n",
      "Images: 407\n",
      "3620 tweets retrieved\n",
      "3630 tweets retrieved\n",
      "Images: 408\n",
      "Images: 409\n",
      "Images: 410\n",
      "3640 tweets retrieved\n",
      "Images: 411\n",
      "3650 tweets retrieved\n",
      "3660 tweets retrieved\n",
      "Images: 412\n",
      "Images: 413\n",
      "3670 tweets retrieved\n",
      "3680 tweets retrieved\n",
      "Images: 414\n",
      "3690 tweets retrieved\n",
      "Images: 415\n",
      "3700 tweets retrieved\n",
      "3710 tweets retrieved\n",
      "3720 tweets retrieved\n",
      "3730 tweets retrieved\n",
      "3740 tweets retrieved\n",
      "3750 tweets retrieved\n",
      "3760 tweets retrieved\n",
      "Images: 416\n",
      "Images: 417\n",
      "3770 tweets retrieved\n",
      "Images: 418\n",
      "3780 tweets retrieved\n",
      "3790 tweets retrieved\n",
      "3800 tweets retrieved\n",
      "Images: 419\n",
      "3810 tweets retrieved\n",
      "3820 tweets retrieved\n",
      "Images: 420\n",
      "3830 tweets retrieved\n",
      "Images: 421\n",
      "Images: 422\n",
      "3840 tweets retrieved\n",
      "Images: 423\n",
      "Images: 424\n",
      "3850 tweets retrieved\n",
      "3860 tweets retrieved\n",
      "Images: 425\n",
      "Images: 426\n",
      "3870 tweets retrieved\n",
      "Images: 427\n",
      "3880 tweets retrieved\n",
      "Images: 428\n",
      "3890 tweets retrieved\n",
      "Images: 429\n",
      "3900 tweets retrieved\n",
      "Images: 430\n",
      "3910 tweets retrieved\n",
      "Images: 431\n",
      "Images: 432\n",
      "3920 tweets retrieved\n",
      "3930 tweets retrieved\n",
      "Images: 433\n",
      "Images: 434\n",
      "3940 tweets retrieved\n",
      "3950 tweets retrieved\n",
      "3960 tweets retrieved\n",
      "3970 tweets retrieved\n",
      "Images: 435\n",
      "3980 tweets retrieved\n",
      "Images: 436\n",
      "3990 tweets retrieved\n",
      "4000 tweets retrieved\n",
      "Images: 437\n",
      "4010 tweets retrieved\n",
      "Images: 438\n",
      "Images: 439\n",
      "4020 tweets retrieved\n",
      "Images: 440\n",
      "Images: 441\n",
      "4030 tweets retrieved\n",
      "4040 tweets retrieved\n",
      "4050 tweets retrieved\n",
      "4060 tweets retrieved\n",
      "Images: 442\n",
      "4070 tweets retrieved\n",
      "Images: 443\n",
      "4080 tweets retrieved\n",
      "Images: 444\n",
      "4090 tweets retrieved\n",
      "Images: 445\n",
      "4100 tweets retrieved\n",
      "4110 tweets retrieved\n",
      "4120 tweets retrieved\n",
      "Images: 446\n",
      "4130 tweets retrieved\n",
      "Images: 447\n",
      "4140 tweets retrieved\n",
      "4150 tweets retrieved\n",
      "4160 tweets retrieved\n",
      "Images: 448\n",
      "4170 tweets retrieved\n",
      "4180 tweets retrieved\n",
      "Images: 449\n",
      "4190 tweets retrieved\n",
      "Images: 450\n",
      "4200 tweets retrieved\n",
      "Images: 451\n",
      "4210 tweets retrieved\n",
      "4220 tweets retrieved\n",
      "Images: 452\n",
      "4230 tweets retrieved\n",
      "Images: 453\n",
      "4240 tweets retrieved\n",
      "Images: 454\n",
      "4250 tweets retrieved\n",
      "Images: 455\n",
      "4260 tweets retrieved\n",
      "Images: 456\n",
      "4270 tweets retrieved\n",
      "4280 tweets retrieved\n",
      "Images: 457\n",
      "Images: 458\n",
      "Images: 459\n",
      "4290 tweets retrieved\n",
      "4300 tweets retrieved\n",
      "Images: 460\n",
      "4310 tweets retrieved\n",
      "Images: 461\n",
      "Images: 462\n",
      "Images: 463\n",
      "4320 tweets retrieved\n",
      "4330 tweets retrieved\n",
      "Images: 464\n",
      "4340 tweets retrieved\n",
      "4350 tweets retrieved\n",
      "Images: 465\n",
      "4360 tweets retrieved\n",
      "Images: 466\n",
      "4370 tweets retrieved\n",
      "4380 tweets retrieved\n",
      "4390 tweets retrieved\n",
      "Images: 467\n",
      "4400 tweets retrieved\n",
      "Images: 468\n",
      "4410 tweets retrieved\n",
      "Images: 469\n",
      "4420 tweets retrieved\n",
      "Images: 470\n",
      "Images: 471\n",
      "Images: 472\n",
      "Images: 473\n",
      "4430 tweets retrieved\n",
      "4440 tweets retrieved\n",
      "4450 tweets retrieved\n",
      "4460 tweets retrieved\n",
      "Images: 474\n",
      "4470 tweets retrieved\n",
      "Images: 475\n",
      "4480 tweets retrieved\n",
      "Images: 476\n",
      "4490 tweets retrieved\n",
      "Images: 477\n",
      "4500 tweets retrieved\n",
      "Images: 478\n",
      "Images: 479\n",
      "4510 tweets retrieved\n",
      "4520 tweets retrieved\n",
      "4530 tweets retrieved\n",
      "Images: 480\n",
      "4540 tweets retrieved\n",
      "Images: 481\n",
      "4550 tweets retrieved\n",
      "4560 tweets retrieved\n",
      "Images: 482\n",
      "4570 tweets retrieved\n",
      "Images: 483\n",
      "4580 tweets retrieved\n",
      "4590 tweets retrieved\n",
      "4600 tweets retrieved\n",
      "Images: 484\n",
      "4610 tweets retrieved\n",
      "4620 tweets retrieved\n",
      "Images: 485\n",
      "Images: 486\n",
      "4630 tweets retrieved\n",
      "Images: 487\n",
      "Images: 488\n",
      "4640 tweets retrieved\n",
      "Images: 489\n",
      "Images: 490\n",
      "Images: 491\n",
      "4650 tweets retrieved\n",
      "Images: 492\n",
      "Images: 493\n",
      "4660 tweets retrieved\n",
      "4670 tweets retrieved\n",
      "Images: 494\n",
      "Images: 495\n",
      "4680 tweets retrieved\n",
      "4690 tweets retrieved\n",
      "Images: 496\n",
      "Images: 497\n",
      "4700 tweets retrieved\n",
      "Images: 498\n",
      "Images: 499\n",
      "Images: 500\n",
      "4710 tweets retrieved\n",
      "Images: 501\n",
      "Images: 502\n",
      "4720 tweets retrieved\n",
      "Images: 503\n",
      "Images: 504\n",
      "4730 tweets retrieved\n",
      "Images: 505\n",
      "4740 tweets retrieved\n",
      "4750 tweets retrieved\n",
      "Images: 506\n",
      "4760 tweets retrieved\n",
      "Images: 507\n",
      "Images: 508\n",
      "4770 tweets retrieved\n",
      "Images: 509\n",
      "Images: 510\n",
      "4780 tweets retrieved\n",
      "Images: 511\n",
      "Images: 512\n",
      "Images: 513\n",
      "4790 tweets retrieved\n",
      "Images: 514\n",
      "4800 tweets retrieved\n",
      "Images: 515\n",
      "4810 tweets retrieved\n",
      "4820 tweets retrieved\n",
      "4830 tweets retrieved\n",
      "4840 tweets retrieved\n",
      "Images: 516\n",
      "Images: 517\n",
      "4850 tweets retrieved\n",
      "4860 tweets retrieved\n",
      "4870 tweets retrieved\n",
      "Images: 518\n",
      "Images: 519\n",
      "4880 tweets retrieved\n",
      "4890 tweets retrieved\n",
      "Images: 520\n",
      "4900 tweets retrieved\n",
      "Images: 521\n",
      "Images: 522\n",
      "4910 tweets retrieved\n",
      "Images: 523\n",
      "Images: 524\n",
      "4920 tweets retrieved\n",
      "Images: 525\n",
      "4930 tweets retrieved\n",
      "Got all the tweets!\n"
     ]
    }
   ],
   "source": [
    "df = get_tweets(bearer_token='AAAAAAAAAAAAAAAAAAAAAGnaTwEAAAAAhRdM6yLmei6skyaWcjbx8IDFnlw%3DLPQHO2CTw1nVjjHLx3htgP9qmeCOgPpt96EdDujokNcWljI5iP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Cleaning and Engineering\n",
    "* The cleaning steps should include any basic steps needed to prepare the data including:\n",
    "    * Binning rare group values\n",
    "    * Standardizing values\n",
    "    * Adjusting for skewness\n",
    "    * Handle missing values\n",
    "* These steps will be unique to each dataset\n",
    "* Engineering includes converting the unstructured text and images features into usable features. This could include:\n",
    "    * Topic moding text\n",
    "    * Extracting characteristics of the text (e.g. word counts, sentiment)\n",
    "    * Extracting characteristics of the images (e..g number of faces, smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin rare group values\n",
    "def bin_groups(df, percent=.05, cols_to_exclude=[]):\n",
    "    import pandas as pd\n",
    "    for col in df:\n",
    "        if col not in cols_to_exclude:\n",
    "            if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "                for group, count in df[col].value_counts().iteritems():\n",
    "                    if count / len(df) < percent:\n",
    "                        df.loc[df[col] == group, col] = 'Other'\n",
    "    return df\n",
    "\n",
    "# Standardize data\n",
    "\n",
    "\n",
    "def standardize_values(df):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn import preprocessing\n",
    "\n",
    "    # Scale/normalize the features\n",
    "    df = pd.DataFrame(preprocessing.MinMaxScaler(\n",
    "    ).fit_transform(df), columns=df.columns)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Dummy code categorical variables\n",
    "\n",
    "\n",
    "def dummy_code_categorical_variables(df):\n",
    "    import pandas as pd\n",
    "\n",
    "    for col in df:\n",
    "        if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "            df = df.join(pd.get_dummies(\n",
    "                df[col], prefix=col, drop_first=True, lsuffix='_left', rsuffix='_right'))\n",
    "\n",
    "    return df\n",
    "\n",
    "# Handle missing values\n",
    "\n",
    "\n",
    "def drop_columns_missing_data(df, cutoff=.5):\n",
    "    import pandas as pd\n",
    "    for col in df:\n",
    "        if df[col].isna().sum() / len(df) > cutoff:\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "    return df\n",
    "\n",
    "# Handle missing values\n",
    "\n",
    "\n",
    "def impute_mean(df):\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    for col in df:\n",
    "        if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "            df = pd.get_dummies(df, columns=[col], drop_first=True)\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    df = pd.DataFrame(imp.fit_transform(df), columns=df.columns)\n",
    "    return df\n",
    "\n",
    "# Handle missing values AND standardize values\n",
    "\n",
    "\n",
    "def impute_KNN(df):\n",
    "    from sklearn.impute import KNNImputer\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    import pandas as pd\n",
    "    for col in df:\n",
    "        if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "            df = pd.get_dummies(df, columns=[col], drop_first=True)\n",
    "    df = pd.DataFrame(MinMaxScaler().fit_transform(df), columns=df.columns)\n",
    "    imp = KNNImputer(n_neighbors=5, weights=\"uniform\")\n",
    "    df = pd.DataFrame(imp.fit_transform(df), columns=df.columns)\n",
    "    return df\n",
    "\n",
    "# Handle missing values\n",
    "\n",
    "\n",
    "def impute_reg(df):\n",
    "    from sklearn.experimental import enable_iterative_imputer\n",
    "    from sklearn.impute import IterativeImputer\n",
    "    import pandas as pd\n",
    "    for col in df:\n",
    "        if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "            df = pd.get_dummies(df, columns=[col], drop_first=True)\n",
    "    imp = IterativeImputer(max_iter=10, random_state=12345)\n",
    "    df = pd.DataFrame(imp.fit_transform(df), columns=df.columns)\n",
    "    return df\n",
    "\n",
    "\n",
    "def fit_mlr(df, test_size=.2, random_state=12345, label=''):\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import pandas as pd\n",
    "    X = df.drop(label, axis=1)\n",
    "    y = df[label]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state)\n",
    "    model = LinearRegression().fit(X_train, y_train)\n",
    "    print(f'R-squared (mlr): \\t{model.score(X_test, y_test)}')\n",
    "    return model\n",
    "\n",
    "\n",
    "def fit_crossvalidate_mlr(df, k, label, repeat=True):\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.model_selection import KFold, RepeatedKFold, cross_val_score\n",
    "    import pandas as pd\n",
    "    from numpy import mean, std\n",
    "    X = df.drop(label, axis=1)\n",
    "    y = df[label]\n",
    "    if repeat:\n",
    "        cv = RepeatedKFold(n_splits=10, n_repeats=5, random_state=12345)\n",
    "    else:\n",
    "        cv = KFold(n_splits=10, random_state=12345, shuffle=True)\n",
    "    scores = cross_val_score(LinearRegression(), X, y,\n",
    "                             scoring='r2', cv=cv, n_jobs=-1)\n",
    "    print(f'Average R-squared:\\t{mean(scores)}')\n",
    "    return LinearRegression().fit(X, y)\n",
    "\n",
    "\n",
    "def calc_sentiment(df):\n",
    "    import pandas as pd\n",
    "    import nltk\n",
    "    from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "    nltk.download('vader_lexicon')\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    df['sentiment_overall'] = 0.0\n",
    "    df['sentiment_negative'] = 0.0\n",
    "    df['sentiment_neutral'] = 0.0\n",
    "    df['sentiment_positive'] = 0.0\n",
    "\n",
    "    for row in df.itertuples():\n",
    "        sentiment = sia.polarity_scores(row.text)\n",
    "        df.at[row.Index, 'sentiment_overall'] = sentiment['compound']\n",
    "        df.at[row.Index, 'sentiment_negative'] = sentiment['neg']\n",
    "        df.at[row.Index, 'sentiment_neutral'] = sentiment['neu']\n",
    "        df.at[row.Index, 'sentiment_positive'] = sentiment['pos']\n",
    "\n",
    "    # df.drop(columns=['Sentiment'], inplace=True) # I don't think this is necessary\n",
    "    return df\n",
    "\n",
    "\n",
    "def image_classification(df, api_key, api_secret):\n",
    "    import pandas as pd\n",
    "    import requests\n",
    "    import json\n",
    "    df_imagga = pd.DataFrame(columns=[\"interior objects\", \"nature landscape\", \"beaches seaside\", \"events parties\", \"food drinks\",\n",
    "                                      \"paintings art\", \"pets animals\", \"text visuals\", \"sunrises sunsets\", \"cars vehicles\",\n",
    "                                      \"macro flowers\", \"streetview architecture\", \"people portraits\"])\n",
    "    for row in df.itertuples():\n",
    "        tweet_id = row.tweet_id\n",
    "        if pd.isnull(row.image_url) or pd.isna(row.image_url):\n",
    "            scores = [0.0] * len(df_imagga.columns)\n",
    "\n",
    "            for n, col in enumerate(df_imagga.columns):\n",
    "                # Iterate through each category of the result\n",
    "                scores[n] = 0.0\n",
    "                # Store the list as a new row in the DataFrame\n",
    "                df_imagga.loc[tweet_id] = scores\n",
    "        else:\n",
    "            url = 'https://api.imagga.com/v2/categories/personal_photos/?image_url=' + row.image_url\n",
    "            request = requests.get(url, auth=(api_key, api_secret))\n",
    "            json_data = json.loads(request.text)\n",
    "\n",
    "            # Create a list of 0.0 scores to update as we get data for each category we want to score in our DataFrame\n",
    "            scores = [0.0] * len(df_imagga.columns)\n",
    "\n",
    "            # Find the associated column in the DataFrame\n",
    "            for n, col in enumerate(df_imagga.columns):\n",
    "                # Iterate through each category of the result\n",
    "                try:\n",
    "                    for category in json_data[\"result\"][\"categories\"]:\n",
    "                        if col == category['name']['en']:\n",
    "                            # Store the score\n",
    "                            scores[n] = category['confidence']\n",
    "                            break  # No need to keep looping once we've found the score\n",
    "\n",
    "                except:\n",
    "                    pass\n",
    "                # Store the list as a new row in the DataFrame\n",
    "                df_imagga.loc[tweet_id] = scores\n",
    "\n",
    "    # merge the two DataFrames\n",
    "    df = pd.merge(df, df_imagga, left_on=df.tweet_id, right_on=df_imagga.index)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def drop_random_columns(df):\n",
    "    if 'Unnamed: 0' in df.columns:\n",
    "        df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    if 'key_0' in df.columns:\n",
    "        df.drop(columns=['key_0'], inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def correct_skew(df):\n",
    "    import numpy as np\n",
    "    for col in df:\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            print(col + \" Normal: \" + str(df[col].skew()))\n",
    "            if (df[col].skew() > 1) or (df[col].skew() < -1):\n",
    "                log = np.log(df[col] + 1).skew()\n",
    "                print(log)\n",
    "                cbrt = (df[col]**(1/3)).skew()\n",
    "                print(cbrt)\n",
    "                sqrt = (df[col]**(1/2)).skew()\n",
    "                print(sqrt)\n",
    "                lowest = min(abs(log), abs(cbrt), abs(sqrt))\n",
    "                print(\"Lowest: \" + str(lowest))\n",
    "                if abs(log) == lowest:\n",
    "                    df[col] = np.log(df[col] + 1)\n",
    "                elif abs(cbrt) == lowest:\n",
    "                    df[col] = df[col]**(1/3)\n",
    "                elif abs(sqrt) == lowest:\n",
    "                    df[col] == df[col]**(1/2)\n",
    "                print(col + \" Fixed :\" + str(df[col].skew()))\n",
    "    for col in df:\n",
    "        if pd.api.types.is_numeric_dtype(col):\n",
    "            print(col + \"Final: \" + str(df[col].skew()))\n",
    "    return df\n",
    "\n",
    "\n",
    "def vif(df):\n",
    "    import pandas as pd\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "\n",
    "    # initialize dictionaries\n",
    "    vif_dict, tolerance_dict = {}, {}\n",
    "\n",
    "    # drop the following columns with high VIF scores (check manually each time :))\n",
    "    for col in df.drop(columns=['key_0', 'tweet_id', 'author_id', 'retweet_count', 'sentiment_neutral', 'sentiment_positive', 'sentiment_negative',\n",
    "                                'cars vehicles', 'sunrises sunsets', 'macro flowers', 'text visuals', 'streetview architecture', 'pets animals',\n",
    "                                'interior objects', 'nature landscape', 'beaches seaside', 'events parties', 'food drinks', 'paintings art', 'people portraits']):\n",
    "\n",
    "        y = df[col]\n",
    "        X = df.drop(columns=[col])\n",
    "\n",
    "        # extract r-squared from the fit\n",
    "        r_squared = LinearRegression().fit(X, y).score(X, y)\n",
    "\n",
    "        # calculate VIF\n",
    "        if r_squared < 1:  # Prevent division by zero runtime error\n",
    "            vif = 1/(1 - r_squared)\n",
    "        else:\n",
    "            vif = 100\n",
    "\n",
    "    df = df.drop(columns=['key_0', 'tweet_id', 'author_id', 'retweet_count', 'sentiment_neutral', 'sentiment_positive', 'sentiment_negative',\n",
    "                              'cars vehicles', 'sunrises sunsets', 'macro flowers', 'text visuals', 'streetview architecture', 'pets animals',\n",
    "                              'interior objects', 'nature landscape', 'beaches seaside', 'events parties', 'food drinks', 'paintings art', 'people portraits'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\isaac\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key_0 Normal: 0.29764168400675634\n",
      "Unnamed: 0 Normal: 0.29764168400675634\n",
      "tweet_id Normal: 0.29764168400675634\n",
      "author_id Normal: -0.11674020206365295\n",
      "retweet_count Normal: 64.3855923893964\n",
      "-0.011005097893568053\n",
      "1.3350914836303844\n",
      "5.9288101826518815\n",
      "Lowest: 0.011005097893568053\n",
      "retweet_count Fixed :-0.011005097893568053\n",
      "like_count Normal: 33.004475627395514\n",
      "5.346717314146055\n",
      "6.976688528670577\n",
      "13.724141863082082\n",
      "Lowest: 5.346717314146055\n",
      "like_count Fixed :5.346717314146055\n",
      "possibly_sensitive Normal: 9.679614800558376\n",
      "9.679614800558381\n",
      "9.679614800558376\n",
      "9.679614800558376\n",
      "Lowest: 9.679614800558376\n",
      "possibly_sensitive Fixed :9.679614800558376\n",
      "sentiment_overall Normal: -0.3552768462325093\n",
      "sentiment_negative Normal: 1.7020968619756087\n",
      "1.4898054213490677\n",
      "0.38190561046081917\n",
      "0.5750123951253653\n",
      "Lowest: 0.38190561046081917\n",
      "sentiment_negative Fixed :0.38190561046081917\n",
      "sentiment_neutral Normal: -0.5063563512837265\n",
      "sentiment_positive Normal: 0.5635930446142006\n",
      "interior objects Normal: 21.18417427637792\n",
      "12.945801278026417\n",
      "10.794814865819166\n",
      "13.378712213647677\n",
      "Lowest: 10.794814865819166\n",
      "interior objects Fixed :10.794814865819166\n",
      "nature landscape Normal: 13.671760582576798\n",
      "11.363659291871667\n",
      "11.318044536353035\n",
      "12.355964117314036\n",
      "Lowest: 11.318044536353035\n",
      "nature landscape Fixed :11.318044536353035\n",
      "beaches seaside Normal: 16.31777283144257\n",
      "12.819763366552928\n",
      "10.72767417403578\n",
      "13.318079268287965\n",
      "Lowest: 10.72767417403578\n",
      "beaches seaside Fixed :10.72767417403578\n",
      "events parties Normal: 5.875718332965474\n",
      "4.249910166121581\n",
      "4.255024284572179\n",
      "4.803508025683032\n",
      "Lowest: 4.249910166121581\n",
      "events parties Fixed :4.249910166121581\n",
      "food drinks Normal: 7.109949221419334\n",
      "5.007857716521281\n",
      "5.066144786265353\n",
      "5.6217891991007525\n",
      "Lowest: 5.007857716521281\n",
      "food drinks Fixed :5.007857716521281\n",
      "paintings art Normal: 12.876353874328274\n",
      "8.70551018481688\n",
      "8.27155263664294\n",
      "10.711903048580126\n",
      "Lowest: 8.27155263664294\n",
      "paintings art Fixed :8.27155263664294\n",
      "pets animals Normal: 26.122953726412614\n",
      "14.285301196202335\n",
      "12.408383316675279\n",
      "15.467493934610518\n",
      "Lowest: 12.408383316675279\n",
      "pets animals Fixed :12.408383316675279\n",
      "text visuals Normal: 11.576188064307418\n",
      "8.734358738950188\n",
      "8.904304372672431\n",
      "9.968345550881132\n",
      "Lowest: 8.734358738950188\n",
      "text visuals Fixed :8.734358738950188\n",
      "sunrises sunsets Normal: 70.19971509913692\n",
      "70.19971509913701\n",
      "70.19971509913707\n",
      "70.19971509913697\n",
      "Lowest: 70.19971509913697\n",
      "sunrises sunsets Fixed :70.19971509913692\n",
      "cars vehicles Normal: 17.121469481965217\n",
      "12.832107682124935\n",
      "12.717465816418539\n",
      "14.608357257761941\n",
      "Lowest: 12.717465816418539\n",
      "cars vehicles Fixed :12.717465816418539\n",
      "macro flowers Normal: 13.304925613954824\n",
      "10.702163831382363\n",
      "10.694958682165481\n",
      "11.949571697727574\n",
      "Lowest: 10.694958682165481\n",
      "macro flowers Fixed :10.694958682165481\n",
      "streetview architecture Normal: 31.72711823503468\n",
      "23.501111014678607\n",
      "22.980844082598896\n",
      "27.700857319442015\n",
      "Lowest: 22.980844082598896\n",
      "streetview architecture Fixed :22.980844082598896\n",
      "people portraits Normal: 5.359330254925507\n",
      "4.1265946027074465\n",
      "4.188579039111087\n",
      "4.569447263309783\n",
      "Lowest: 4.1265946027074465\n",
      "people portraits Fixed :4.1265946027074465\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>like_count</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>sentiment_overall</th>\n",
       "      <th>language_en</th>\n",
       "      <th>language_es</th>\n",
       "      <th>source_Twitter Web App</th>\n",
       "      <th>source_Twitter for Android</th>\n",
       "      <th>source_Twitter for iPhone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.486827</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.999796</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.381077</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.999724</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.381077</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.999189</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.707162</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.998832</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.707162</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4923</th>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.617512</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4924</th>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.381077</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4925</th>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.707162</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4926</th>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.707162</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4927</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.825876</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4928 rows  9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  like_count  possibly_sensitive  sentiment_overall  \\\n",
       "0       1.000000         0.0                 0.0           0.486827   \n",
       "1       0.999796         0.0                 0.0           0.381077   \n",
       "2       0.999724         0.0                 0.0           0.381077   \n",
       "3       0.999189         0.0                 0.0           0.707162   \n",
       "4       0.998832         0.0                 0.0           0.707162   \n",
       "...          ...         ...                 ...                ...   \n",
       "4923    0.000372         0.0                 0.0           0.617512   \n",
       "4924    0.000250         0.0                 0.0           0.381077   \n",
       "4925    0.000141         0.0                 0.0           0.707162   \n",
       "4926    0.000110         0.0                 0.0           0.707162   \n",
       "4927    0.000000         0.0                 0.0           0.825876   \n",
       "\n",
       "      language_en  language_es  source_Twitter Web App  \\\n",
       "0             0.0          0.0                     0.0   \n",
       "1             1.0          0.0                     0.0   \n",
       "2             1.0          0.0                     0.0   \n",
       "3             0.0          0.0                     0.0   \n",
       "4             1.0          0.0                     0.0   \n",
       "...           ...          ...                     ...   \n",
       "4923          1.0          0.0                     1.0   \n",
       "4924          1.0          0.0                     0.0   \n",
       "4925          1.0          0.0                     0.0   \n",
       "4926          1.0          0.0                     0.0   \n",
       "4927          1.0          0.0                     0.0   \n",
       "\n",
       "      source_Twitter for Android  source_Twitter for iPhone  \n",
       "0                            1.0                        0.0  \n",
       "1                            1.0                        0.0  \n",
       "2                            1.0                        0.0  \n",
       "3                            0.0                        1.0  \n",
       "4                            0.0                        0.0  \n",
       "...                          ...                        ...  \n",
       "4923                         0.0                        0.0  \n",
       "4924                         1.0                        0.0  \n",
       "4925                         0.0                        1.0  \n",
       "4926                         1.0                        0.0  \n",
       "4927                         0.0                        1.0  \n",
       "\n",
       "[4928 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./tweets.csv\")\n",
    "\n",
    "# Data cleaning and Prep\n",
    "df = bin_groups(df, percent=.05, cols_to_exclude=['text', 'created_at', 'image_url']) # Exclude columns it doesn't make sense to bin\n",
    "\n",
    "# Engineering\n",
    "df = calc_sentiment(df)\n",
    "df = image_classification(df=df, api_key='acc_81f8f349aa51d69', api_secret='dd79216824834fc2b208323818d1ca35')\n",
    "\n",
    "# drop columns I'm finished analyzing\n",
    "df = df.drop(columns=['text', 'created_at', 'image_url'])\n",
    "\n",
    "# INSERT SKEW FUNCTION HERE\n",
    "df = correct_skew(df)\n",
    "\n",
    "# impute KNN, also standardize values\n",
    "df = impute_KNN(df)\n",
    "\n",
    "# INSERT VIF FUNCTION HERE\n",
    "df = vif(df)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling \n",
    "(same requirements of the modeling project)\n",
    "* Generate the best possible model for:\n",
    "    * Regression\n",
    "    * Classification\n",
    "    * Clustering\n",
    "* You choose which features to keep in the model model\n",
    "* There is no required level of fit metric. Your task is simply to get the best fit metrics possible--not achieve a certain value. All datasets are different and there is no way to compare them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_selection(df, cols_to_drop=[]):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn import model_selection\n",
    "    from sklearn import preprocessing\n",
    "    import sklearn.neural_network as nn\n",
    "    from sklearn.linear_model import RidgeCV, LassoCV\n",
    "    import sklearn.ensemble as se\n",
    "    import sklearn.tree as tree\n",
    "    from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n",
    "    from sklearn import gaussian_process\n",
    "    from sklearn import neighbors\n",
    "    from sklearn import svm\n",
    "    import sklearn.linear_model as lm\n",
    "    import pickle\n",
    "\n",
    "    df = df.select_dtypes(np.number)  # Remove categorical features first\n",
    "    y = df.like_count                    # Save the label first\n",
    "    # Remove the label from the feature list\n",
    "    X = df.drop(columns=cols_to_drop)\n",
    "\n",
    "    # Scale/normalize the features\n",
    "    X = pd.DataFrame(preprocessing.MinMaxScaler(\n",
    "    ).fit_transform(X), columns=X.columns)\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.3, random_state=12345)\n",
    "\n",
    "    # Eyeball the data to make sure it looks right:\n",
    "    X_train\n",
    "\n",
    "    fit = {}  # Use this to store each of the fit metrics\n",
    "\n",
    "    # 1. LINEAR MODELS: assumes normal distribution, homoscedasticity, no multi-collinearity, independence, and no auto-correlation (some exceptions apply)\n",
    "\n",
    "    # 1.1. Ordinary Least Squares Multiple Linear Regression\n",
    "    model_ols = lm.LinearRegression()\n",
    "    model_ols.fit(X_train, y_train)\n",
    "    fit['OrdinaryLS R'] = model_ols.score(X_test, y_test)\n",
    "\n",
    "    # 1.2. Ridge Regression: more robust to multi-collinearity\n",
    "    # adjust this alpha parameter for better results (between 0 and 1)\n",
    "    model_rr = lm.Ridge(alpha=0.5)\n",
    "    model_rr.fit(X_train, y_train)\n",
    "    fit['Ridge R'] = model_rr.score(X_test, y_test)\n",
    "\n",
    "    # 1.3. Lasso Regression: better for sparse values like RetweetCount where most are zeros but a few have many retweets.\n",
    "    # adjust this alpha parameter for better results (between 0 and 1)\n",
    "    model_lr = lm.Lasso(alpha=0.1)\n",
    "    model_lr.fit(X_train, y_train)\n",
    "    fit['Lasso R'] = model_lr.score(X_test, y_test)\n",
    "\n",
    "    # 1.4. Least Angle Regression: good when the number of features is greater than the number of samples\n",
    "    # adjust this alpha parameter for better results (between 0 and 1)\n",
    "    model_llr = lm.LassoLars(alpha=0.1)\n",
    "    model_llr.fit(X_train, y_train)\n",
    "    fit['LARS Lasso R'] = model_llr.score(X_test, y_test)\n",
    "\n",
    "    # 1.5. Bayesian Regression: probability based; allows regularization parameters, automatically tuned to data\n",
    "    model_br = lm.BayesianRidge()\n",
    "    model_br.fit(X_train, y_train)\n",
    "    fit['Bayesian R'] = model_br.score(X_test, y_test)\n",
    "\n",
    "    # SUPPORT VECTOR MACHINES\n",
    "    # 1.9. SVM: this is the default SVM, parameters can be modified to make this more accurate\n",
    "    model_svm = svm.SVR()\n",
    "    model_svm.fit(X_train, y_train)\n",
    "    fit['SupportVM R'] = model_svm.score(X_test, y_test)\n",
    "\n",
    "    # 1.10. Linear SVM: Faster than SVM but only considers a linear model\n",
    "    model_lsvm = svm.LinearSVR()\n",
    "    model_lsvm.fit(X_train, y_train)\n",
    "    fit['Linear SVM R'] = model_lsvm.score(X_test, y_test)\n",
    "\n",
    "    # 1.11. NuSVM:\n",
    "    model_nusvm = svm.NuSVR()\n",
    "    model_nusvm.fit(X_train, y_train)\n",
    "    fit['NuSupportVM R'] = model_nusvm.score(X_test, y_test)\n",
    "\n",
    "    # STOCHASTIC GRADIENT DESCENT REGRESSION\n",
    "    # 1.12. SGDRegressor:\n",
    "    model_sgdr = lm.SGDRegressor()\n",
    "    model_sgdr.fit(X_train, y_train)\n",
    "    fit['SGradientD R'] = model_sgdr.score(X_test, y_test)\n",
    "\n",
    "    # KNN: NEAREST NEIGHBORS REGRESSION\n",
    "\n",
    "    # 1.13. KNeighborsRegressor:\n",
    "    # model_knnr = neighbors.KNeighborsRegressor(5, 'uniform')\n",
    "    # model_knnr.fit(X_train, y_train)\n",
    "    # fit['KNNeighbors R'] = model_knnr.score(X_test, y_test)\n",
    "\n",
    "    # 1.14. KNeighborsRegressor:\n",
    "    # model_knnrd = neighbors.KNeighborsRegressor(8, 'distance')\n",
    "    # model_knnrd.fit(X_train, y_train)\n",
    "    # fit['KNNeighborsD R'] = model_knnrd.score(X_test, y_test)\n",
    "\n",
    "    # GAUSSIAN PROCESS REGRESSION\n",
    "\n",
    "    # 1.15. GaussianProcessRegressor:\n",
    "    model_gpr = gaussian_process.GaussianProcessRegressor(\n",
    "        DotProduct() + WhiteKernel())\n",
    "    model_gpr.fit(X_train, y_train)\n",
    "    fit['GaussianP R'] = model_gpr.score(X_test, y_test)\n",
    "\n",
    "    # Sort and print the dictionary by greatest R squared to least\n",
    "    r2s = sorted_list_by_value = sorted(fit, key=fit.__getitem__, reverse=True)\n",
    "    for r2 in r2s:\n",
    "        print(f'{r2}:\\t{fit[r2]}')\n",
    "\n",
    "    # Select the model with the highest R squared\n",
    "    print(f'Best Regression Model: {r2s[0]} (R2:{fit[r2s[0]]})')\n",
    "    print('_____________________________________________')\n",
    "    model = fit[r2s[1]]\n",
    "    type(model)\n",
    "\n",
    "    # Save the model with the highest fit metric\n",
    "    pickle.dump(model, open('regression_model.sav', 'wb'))\n",
    "\n",
    "def classification_selection(df, cols_to_drop=[]):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn import model_selection\n",
    "    from sklearn import preprocessing\n",
    "    import sklearn.neural_network as nn\n",
    "    from sklearn.linear_model import RidgeCV, LassoCV\n",
    "    import sklearn.ensemble as se\n",
    "    import sklearn.tree as tree\n",
    "    from sklearn import svm\n",
    "    import pickle\n",
    "\n",
    "    df = df.select_dtypes(np.number)  # Remove categorical features first\n",
    "    y = df.like_count                    # Save the label first\n",
    "    # Remove the label from the feature list\n",
    "    X = df.drop(columns=cols_to_drop)\n",
    "\n",
    "    # Scale/normalize the features\n",
    "    X = pd.DataFrame(preprocessing.MinMaxScaler(\n",
    "    ).fit_transform(X), columns=X.columns)\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "        X, y, test_size=0.3, random_state=12345)\n",
    "\n",
    "    # Eyeball the data to make sure it looks right:\n",
    "    X_train\n",
    "\n",
    "    fit = {}  # Use this to store each of the fit metrics\n",
    "    # DECISION TREE MODELS: no assumptions about the data\n",
    "\n",
    "    # 1.16. Decision Tree Regression\n",
    "    model_dt = tree.DecisionTreeRegressor(random_state=12345)\n",
    "    model_dt.fit(X_train, y_train)\n",
    "    fit['Dec Tree R'] = model_dt.score(X_test, y_test)\n",
    "\n",
    "    # DECISION TREE-BASED ENSEMBLE MODELS: great for minimizing overfitting, these are based on averaging many unique sub-samples and combining algorithms\n",
    "    # 1.17. Decision Forrest\n",
    "    model_df = se.RandomForestRegressor(random_state=12345)\n",
    "    model_df.fit(X_train, y_train)\n",
    "    fit['Dec Forest R'] = model_df.score(X_test, y_test)\n",
    "\n",
    "    # 1.18. ExtraTreesRegressor\n",
    "    model_etr = se.ExtraTreesRegressor(random_state=12345)\n",
    "    model_etr.fit(X_train, y_train)\n",
    "    fit['Extra Trees R'] = model_etr.score(X_test, y_test)\n",
    "\n",
    "    # 1.19. AdaBoostRegressor\n",
    "    model_abr = se.AdaBoostRegressor(n_estimators=100, random_state=12345)\n",
    "    model_abr.fit(X_train, y_train)\n",
    "    fit['AdaBoost DT R'] = model_abr.score(X_test, y_test)\n",
    "\n",
    "    # 1.20. GradientBoostingRegressor\n",
    "    model_gbr = se.GradientBoostingRegressor(random_state=12345)\n",
    "    model_gbr.fit(X_train, y_train)\n",
    "    fit['Grad. Boost R'] = model_gbr.score(X_test, y_test)\n",
    "\n",
    "    # 1.22. VotingRegressor: will combine other algorithms into an average; kind of cool\n",
    "    model_vr = se.VotingRegressor(estimators=[('DT', model_dt), ('DF', model_df), (\n",
    "        'ETR', model_etr), ('ABR', model_abr), ('GBR', model_gbr)])\n",
    "    model_vr.fit(X_train, y_train)\n",
    "    fit['Voting R'] = model_vr.score(X_test, y_test)\n",
    "\n",
    "    # 1.23. StackingRegressor\n",
    "    estimators = [('ridge', RidgeCV()), ('lasso', LassoCV(\n",
    "        random_state=42)), ('svr', svm.SVR(C=1, gamma=1e-6))]\n",
    "    model_sr = se.StackingRegressor(\n",
    "        estimators=estimators, final_estimator=se.GradientBoostingRegressor(random_state=12345))\n",
    "    model_sr.fit(X_train, y_train)\n",
    "    fit['Stacking R'] = model_sr.score(X_test, y_test)\n",
    "\n",
    "    # NEURAL-NETWORK MODELS: Based on deep learning methods\n",
    "\n",
    "    # 1.24. MLPRegressor\n",
    "    # Turn max_iter way up or down to get a more accurate result\n",
    "    model_nn = nn.MLPRegressor(max_iter=1000, random_state=12345)\n",
    "    model_nn.fit(X_train, y_train)\n",
    "    fit['NeuralNet R'] = model_nn.score(X_test, y_test)\n",
    "\n",
    "    # Sort and print the dictionary by greatest R squared to least\n",
    "    r2s = sorted_list_by_value = sorted(fit, key=fit.__getitem__, reverse=True)\n",
    "    for r2 in r2s:\n",
    "        print(f'{r2}:\\t{fit[r2]}')\n",
    "\n",
    "    # Select the model with the highest R squared\n",
    "    print(f'Best Classification Model: {r2s[0]} (R2:{fit[r2s[0]]})')\n",
    "    print('_____________________________________________')\n",
    "    model = fit[r2s[1]]\n",
    "    type(model)\n",
    "\n",
    "    # Save the model with the highest fit metric\n",
    "    pickle.dump(model, open('classification_model.sav', 'wb'))\n",
    "\n",
    "\n",
    "def clustering(df, cols_to_drop=[]):\n",
    "    import gower\n",
    "    from sklearn.cluster import AgglomerativeClustering\n",
    "    import pickle\n",
    "\n",
    "    distance_matrix = gower.gower_matrix(df)\n",
    "    agg = AgglomerativeClustering(\n",
    "        affinity='precomputed', linkage='average').fit(distance_matrix)\n",
    "\n",
    "    # make a cluster column\n",
    "    df_wcluster = df.copy()\n",
    "    df_wcluster['cluster'] = agg.labels_\n",
    "    df_wcluster.head()\n",
    "\n",
    "    # Save the model with the highest fit metric\n",
    "    pickle.dump(df_wcluster, open('clustering_model.sav', 'wb'))\n",
    "\n",
    "    return df_wcluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayesian R:\t0.03186745769860033\n",
      "GaussianP R:\t0.031138226846064376\n",
      "Ridge R:\t0.031100323210905834\n",
      "OrdinaryLS R:\t0.03105437906022601\n",
      "SGradientD R:\t0.02708578075174628\n",
      "Lasso R:\t-0.0001417208352607613\n",
      "LARS Lasso R:\t-0.0001417208352607613\n",
      "NuSupportVM R:\t-0.06973854875277508\n",
      "Linear SVM R:\t-0.0729971205340425\n",
      "SupportVM R:\t-0.7613962840389945\n",
      "Best Regression Model: Bayesian R (R2:0.03186745769860033)\n",
      "_____________________________________________\n",
      "Stacking R:\t0.03100597834087704\n",
      "NeuralNet R:\t0.019399076098472245\n",
      "Grad. Boost R:\t-0.0501865585156982\n",
      "Voting R:\t-0.20462128115134215\n",
      "AdaBoost DT R:\t-0.2351071716658366\n",
      "Dec Forest R:\t-0.24090557145627867\n",
      "Extra Trees R:\t-0.5473773308361485\n",
      "Dec Tree R:\t-1.2681432559569705\n",
      "Best Classification Model: Stacking R (R2:0.03100597834087704)\n",
      "_____________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>like_count</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>sentiment_overall</th>\n",
       "      <th>language_en</th>\n",
       "      <th>language_es</th>\n",
       "      <th>source_Twitter Web App</th>\n",
       "      <th>source_Twitter for Android</th>\n",
       "      <th>source_Twitter for iPhone</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.486827</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.999796</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.381077</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.999724</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.381077</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.999189</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.707162</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.998832</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.707162</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4923</th>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.617512</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4924</th>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.381077</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4925</th>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.707162</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4926</th>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.707162</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4927</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.825876</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4928 rows  10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  like_count  possibly_sensitive  sentiment_overall  \\\n",
       "0       1.000000         0.0                 0.0           0.486827   \n",
       "1       0.999796         0.0                 0.0           0.381077   \n",
       "2       0.999724         0.0                 0.0           0.381077   \n",
       "3       0.999189         0.0                 0.0           0.707162   \n",
       "4       0.998832         0.0                 0.0           0.707162   \n",
       "...          ...         ...                 ...                ...   \n",
       "4923    0.000372         0.0                 0.0           0.617512   \n",
       "4924    0.000250         0.0                 0.0           0.381077   \n",
       "4925    0.000141         0.0                 0.0           0.707162   \n",
       "4926    0.000110         0.0                 0.0           0.707162   \n",
       "4927    0.000000         0.0                 0.0           0.825876   \n",
       "\n",
       "      language_en  language_es  source_Twitter Web App  \\\n",
       "0             0.0          0.0                     0.0   \n",
       "1             1.0          0.0                     0.0   \n",
       "2             1.0          0.0                     0.0   \n",
       "3             0.0          0.0                     0.0   \n",
       "4             1.0          0.0                     0.0   \n",
       "...           ...          ...                     ...   \n",
       "4923          1.0          0.0                     1.0   \n",
       "4924          1.0          0.0                     0.0   \n",
       "4925          1.0          0.0                     0.0   \n",
       "4926          1.0          0.0                     0.0   \n",
       "4927          1.0          0.0                     0.0   \n",
       "\n",
       "      source_Twitter for Android  source_Twitter for iPhone  cluster  \n",
       "0                            1.0                        0.0        0  \n",
       "1                            1.0                        0.0        0  \n",
       "2                            1.0                        0.0        0  \n",
       "3                            0.0                        1.0        0  \n",
       "4                            0.0                        0.0        0  \n",
       "...                          ...                        ...      ...  \n",
       "4923                         0.0                        0.0        0  \n",
       "4924                         1.0                        0.0        0  \n",
       "4925                         0.0                        1.0        0  \n",
       "4926                         1.0                        0.0        0  \n",
       "4927                         0.0                        1.0        0  \n",
       "\n",
       "[4928 rows x 10 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_modeling = df.copy()\n",
    "\n",
    "regression_selection(df_modeling, cols_to_drop=['like_count'])\n",
    "classification_selection(df_modeling, cols_to_drop=['like_count'])\n",
    "df_modeling = clustering(df_modeling)\n",
    "\n",
    "df_modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automation\n",
    "* Dynamically select the best algorithm for your regression and classification models\n",
    "* Demonstrate through the feature importance metric which features should be included in the model. However, you don't need to set up an automated selection of features. You can manually decide which features to include\n",
    "* Dynamically save the best fitting model to a .sav file in the same folder as your .ipynb\n",
    "* Arrange your .ipynb file so that the \"Run all\" command will handle all steps above in order from data collection to .sav file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f8ec8e2802e9ec64c1de1126b52a3a3eba2bbbc7f0465a520b33d3486dfa46c4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
