{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection and Retrieval\n",
    "Data Collection and Retrieval (same requirements as the individual data retrieval project):\n",
    "* Topic: anything you choose, but it must include the following data types:\n",
    "    * numeric features\n",
    "    * categorical features (to be dummy coded)\n",
    "    * text features (to be processed using text analytics)\n",
    "    * image features (to be processed using image classification) \n",
    "    * labels to choose from (i.e. outcomes that you want to predict with the other features)\n",
    "* Scrape approximately ~500 records; +/- 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyLDAvis\n",
    "!pip install pyLDAvis.gensim\n",
    "!pip install logging\n",
    "!pip install nltk\n",
    "!pip install -U pip setuptools wheel\n",
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def get_tweets(bearer_token):\n",
    "    headers = {'Authorization': ('Bearer ' + bearer_token)}\n",
    "\n",
    "    n = 525                               # The total number of tweets we want\n",
    "    max_results = 10 # The number of tweets to pull per request; must be between 10 and 100\n",
    "    total_retrieved = 0                   # To keep track of when to stop\n",
    "    next_token = \"\"                       # Must be empty on first iteration\n",
    "    search_term = \"manchester%20united\"\n",
    "\n",
    "    # Create empty DataFrames and set columns\n",
    "    df_tweets = pd.DataFrame(columns=['tweet_id', 'author_id', 'retweet_count', 'like_count',\n",
    "                                      'text', 'language', 'created_at', 'source', 'possibly_sensitive', 'image_url'])\n",
    "    df_users = pd.DataFrame(columns=['user_id', 'username', 'created_at', 'description', 'profile_image_url',\n",
    "                            'protected', 'verified', 'followers_count', 'following_count', 'tweet_count', 'listed_count'])\n",
    "\n",
    "    # stop when we have n results\n",
    "    while total_retrieved < n:\n",
    "\n",
    "        # the first time through the loop, we do not need the next_token parameter\n",
    "        if next_token == \"\":\n",
    "            url = f'https://api.twitter.com/2/tweets/search/recent?query={search_term}&max_results={max_results}'\n",
    "        else:\n",
    "            url = f'https://api.twitter.com/2/tweets/search/recent?query={search_term}&max_results={max_results}&next_token={next_token}'\n",
    "\n",
    "        # These are the extra parameters we will add to the querystring; we won't store them all though; just want you to see what's possible\n",
    "        url += f'&expansions=geo.place_id,author_id,attachments.media_keys'\n",
    "        url += f'&tweet.fields=attachments,author_id,context_annotations,conversation_id,created_at,entities,geo,id,in_reply_to_user_id,lang,possibly_sensitive,public_metrics,referenced_tweets,reply_settings,source,text,withheld'\n",
    "        url += f'&media.fields=media_key,type,url&user.fields=created_at,description,entities,id,location,name,pinned_tweet_id,profile_image_url,protected,public_metrics,url,username,verified,withheld'\n",
    "\n",
    "        # make the request to the Twitter API Recent Search endpoint\n",
    "        response = requests.request(\"GET\", url, headers=headers)\n",
    "        try:  # Just in case we get an error\n",
    "            json_data = json.loads(response.text)\n",
    "            # print(json_data)\n",
    "        except:\n",
    "            print(response.text)\n",
    "\n",
    "        for tweet in json_data['data']:\n",
    "            media_key = \"\"  # Reset to empty each time through the loop so that we can use it for a condition later\n",
    "\n",
    "            # Store the data into variables\n",
    "            tweet_id = tweet['id']\n",
    "            author_id = tweet['author_id']\n",
    "            retweet_count = tweet['public_metrics']['retweet_count']    # label\n",
    "            like_count = tweet['public_metrics']['like_count']          # label\n",
    "            image_url = \"\"                                              # image\n",
    "            text = tweet['text']                                        # text\n",
    "            created_at = tweet['created_at']                            # categorical\n",
    "            source = tweet['source']                                    # categorical\n",
    "            possibly_sensitive = tweet['possibly_sensitive']            # categorical\n",
    "            language = tweet['lang']                                    # categorical\n",
    "\n",
    "            # Find out if there is media\n",
    "            if 'attachments' in tweet:\n",
    "                if 'media_keys' in tweet['attachments']:\n",
    "                    media_key = tweet['attachments']['media_keys'][0]\n",
    "\n",
    "            # If there is a media key in this tweet, iterate through tweet['includes']['media'] until we find it\n",
    "            if media_key != \"\":\n",
    "                for media in json_data['includes']['media']:\n",
    "                    # Only if the media_key matches the one we stored\n",
    "                    if media['media_key'] == media_key:\n",
    "                        if media['type'] == 'photo':      # Only if it is a photo; ignore videos\n",
    "                            # Store the url in a variable\n",
    "                            image_url = media['url']\n",
    "\n",
    "            # Add the new data to a new record in the DataFrame\n",
    "            df_tweets.loc[tweet_id] = [tweet_id, author_id, retweet_count, like_count,\n",
    "                                       text, language, created_at, source, possibly_sensitive, image_url]\n",
    "\n",
    "        # keep track of how many results have been obtained so far:\n",
    "        total_retrieved += 10\n",
    "        print(f'{total_retrieved} tweets retrieved')\n",
    "\n",
    "        # keep track of where to start next time, but quit if there are no more results\n",
    "        try:\n",
    "            next_token = json_data['meta']['next_token']\n",
    "        except:\n",
    "            break\n",
    "\n",
    "        # get user info\n",
    "        for user in json_data['includes']['users']:\n",
    "            user_id = user['id']\n",
    "            user_name = user['username']\n",
    "            user_created_at = user['created_at']\n",
    "            user_description = user['description']\n",
    "            user_profile_image_url = user['profile_image_url']\n",
    "            user_protected = user['protected']\n",
    "            user_verified = user['verified']\n",
    "            user_followers_count = user['public_metrics']['followers_count']\n",
    "            user_following_count = user['public_metrics']['following_count']\n",
    "            user_tweet_count = user['public_metrics']['tweet_count']\n",
    "            user_listed_count = user['public_metrics']['listed_count']\n",
    "\n",
    "            # put user info into a user dataframe\n",
    "            df_users.loc[user_id] = [user_id, user_name, user_created_at, user_description, user_profile_image_url,\n",
    "                                     user_protected, user_verified, user_followers_count, user_following_count, user_tweet_count, user_listed_count]\n",
    "\n",
    "        # sleep to avoid hitting the rate limit\n",
    "        time.sleep(8)\n",
    "\n",
    "    # All done! save the dataframes to csv files\n",
    "    df_tweets.to_csv('tweets.csv')\n",
    "    df_users.to_csv('users.csv')\n",
    "    print('Got all the tweets!')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_tweets(bearer_token='AAAAAAAAAAAAAAAAAAAAAGnaTwEAAAAAhRdM6yLmei6skyaWcjbx8IDFnlw%3DLPQHO2CTw1nVjjHLx3htgP9qmeCOgPpt96EdDujokNcWljI5iP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Cleaning and Engineering\n",
    "* The cleaning steps should include any basic steps needed to prepare the data including:\n",
    "    * Binning rare group values\n",
    "    * Standardizing values\n",
    "    * Adjusting for skewness\n",
    "    * Handle missing values\n",
    "* These steps will be unique to each dataset\n",
    "* Engineering includes converting the unstructured text and images features into usable features. This could include:\n",
    "    * Topic moding text\n",
    "    * Extracting characteristics of the text (e.g. word counts, sentiment)\n",
    "    * Extracting characteristics of the images (e..g number of faces, smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin rare group values\n",
    "def bin_groups(df, percent=.05, cols_to_exclude=[]):\n",
    "    import pandas as pd\n",
    "    for col in df:\n",
    "        if col not in cols_to_exclude:\n",
    "            if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "                for group, count in df[col].value_counts().iteritems():\n",
    "                    if count / len(df) < percent:\n",
    "                        df.loc[df[col] == group, col] = 'Other'\n",
    "    return df\n",
    "\n",
    "#Standardize data\n",
    "def standardize_values(df):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn import preprocessing\n",
    "\n",
    "    # Scale/normalize the features\n",
    "    df = pd.DataFrame(preprocessing.MinMaxScaler().fit_transform(df), columns=df.columns)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Dummy code categorical variables\n",
    "def dummy_code_categorical_variables(df):\n",
    "    import pandas as pd\n",
    "\n",
    "    for col in df:\n",
    "        if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "            df = df.join(pd.get_dummies(df[col], prefix=col, drop_first=True, lsuffix='_left', rsuffix='_right'))\n",
    "\n",
    "    return df\n",
    "\n",
    "# Handle missing values\n",
    "def drop_columns_missing_data(df, cutoff=.5):\n",
    "    import pandas as pd\n",
    "    for col in df:\n",
    "        if df[col].isna().sum() / len(df) > cutoff:\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "    return df\n",
    "\n",
    "# Handle missing values\n",
    "def impute_mean(df):\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    for col in df:\n",
    "        if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "            df = pd.get_dummies(df, columns=[col], drop_first=True)\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    df = pd.DataFrame(imp.fit_transform(df), columns=df.columns)\n",
    "    return df\n",
    "\n",
    "# Handle missing values AND standardize values\n",
    "def impute_KNN(df):\n",
    "    from sklearn.impute import KNNImputer\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    import pandas as pd\n",
    "    for col in df:\n",
    "        if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "            df = pd.get_dummies(df, columns=[col], drop_first=True)\n",
    "    df = pd.DataFrame(MinMaxScaler().fit_transform(df), columns=df.columns)\n",
    "    imp = KNNImputer(n_neighbors=5, weights=\"uniform\")\n",
    "    df = pd.DataFrame(imp.fit_transform(df), columns=df.columns)\n",
    "    return df\n",
    "\n",
    "# Handle missing values\n",
    "def impute_reg(df):\n",
    "    from sklearn.experimental import enable_iterative_imputer\n",
    "    from sklearn.impute import IterativeImputer\n",
    "    import pandas as pd\n",
    "    for col in df:\n",
    "        if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "            df = pd.get_dummies(df, columns=[col], drop_first=True)\n",
    "    imp = IterativeImputer(max_iter=10, random_state=12345)\n",
    "    df = pd.DataFrame(imp.fit_transform(df), columns=df.columns)\n",
    "    return df\n",
    "\n",
    "\n",
    "def fit_mlr(df, test_size=.2, random_state=12345, label=''):\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import pandas as pd\n",
    "    X = df.drop(label, axis=1)\n",
    "    y = df[label]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state)\n",
    "    model = LinearRegression().fit(X_train, y_train)\n",
    "    print(f'R-squared (mlr): \\t{model.score(X_test, y_test)}')\n",
    "    return model\n",
    "\n",
    "\n",
    "def fit_crossvalidate_mlr(df, k, label, repeat=True):\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.model_selection import KFold, RepeatedKFold, cross_val_score\n",
    "    import pandas as pd\n",
    "    from numpy import mean, std\n",
    "    X = df.drop(label, axis=1)\n",
    "    y = df[label]\n",
    "    if repeat:\n",
    "        cv = RepeatedKFold(n_splits=10, n_repeats=5, random_state=12345)\n",
    "    else:\n",
    "        cv = KFold(n_splits=10, random_state=12345, shuffle=True)\n",
    "    scores = cross_val_score(LinearRegression(), X, y,\n",
    "                             scoring='r2', cv=cv, n_jobs=-1)\n",
    "    print(f'Average R-squared:\\t{mean(scores)}')\n",
    "    return LinearRegression().fit(X, y)\n",
    "\n",
    "def calc_sentiment(df):\n",
    "    import pandas as pd\n",
    "    import nltk\n",
    "    from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "    nltk.download('vader_lexicon')\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    df['sentiment_overall'] = 0.0\n",
    "    df['sentiment_negative'] = 0.0\n",
    "    df['sentiment_neutral'] = 0.0\n",
    "    df['sentiment_positive'] = 0.0\n",
    "\n",
    "    for row in df.itertuples():\n",
    "        sentiment = sia.polarity_scores(row.text)\n",
    "        df.at[row.Index, 'sentiment_overall'] = sentiment['compound']\n",
    "        df.at[row.Index, 'sentiment_negative'] = sentiment['neg']\n",
    "        df.at[row.Index, 'sentiment_neutral'] = sentiment['neu']\n",
    "        df.at[row.Index, 'sentiment_positive'] = sentiment['pos']\n",
    "\n",
    "    # df.drop(columns=['Sentiment'], inplace=True) # I don't think this is necessary\n",
    "    return df\n",
    "\n",
    "def image_classification(df, api_key, api_secret):\n",
    "    import pandas as pd\n",
    "    import requests, json\n",
    "    df_imagga = pd.DataFrame(columns=[\"interior objects\", \"nature landscape\", \"beaches seaside\", \"events parties\", \"food drinks\",\n",
    "    \"paintings art\", \"pets animals\", \"text visuals\", \"sunrises sunsets\", \"cars vehicles\",\n",
    "    \"macro flowers\", \"streetview architecture\", \"people portraits\"])\n",
    "    for row in df.itertuples():\n",
    "        tweet_id = row.tweet_id\n",
    "        if pd.isnull(row.image_url) or pd.isna(row.image_url):\n",
    "            scores = [0.0] * len(df_imagga.columns)\n",
    "\n",
    "            for n, col in enumerate(df_imagga.columns):\n",
    "                # Iterate through each category of the result\n",
    "                scores[n] = 0.0\n",
    "                # Store the list as a new row in the DataFrame\n",
    "                df_imagga.loc[tweet_id] = scores\n",
    "        else:\n",
    "            url = 'https://api.imagga.com/v2/categories/personal_photos/?image_url=' + row.image_url\n",
    "            request = requests.get(url, auth=(api_key, api_secret))\n",
    "            json_data = json.loads(request.text)\n",
    "\n",
    "            # Create a list of 0.0 scores to update as we get data for each category we want to score in our DataFrame\n",
    "            scores = [0.0] * len(df_imagga.columns)\n",
    "\n",
    "            # Find the associated column in the DataFrame\n",
    "            for n, col in enumerate(df_imagga.columns):\n",
    "                # Iterate through each category of the result\n",
    "                for category in json_data[\"result\"][\"categories\"]:\n",
    "                    if col == category['name']['en']:\n",
    "                        # Store the score\n",
    "                        scores[n] = category['confidence']\n",
    "                        break  # No need to keep looping once we've found the score\n",
    "                # Store the list as a new row in the DataFrame\n",
    "                df_imagga.loc[tweet_id] = scores\n",
    "\n",
    "    #merge the two DataFrames\n",
    "    df = pd.merge(df, df_imagga, left_on=df.tweet_id, right_on=df_imagga.index)\n",
    "\n",
    "    return df\n",
    "\n",
    "def drop_random_columns(df):\n",
    "    if 'Unnamed: 0' in df.columns:\n",
    "        df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    if 'key_0' in df.columns:\n",
    "        df.drop(columns=['key_0'], inplace=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Pandas' object has no attribute 'image_url'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_45828/1463396373.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitertuples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mtweet_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtweet_id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_url\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_imagga\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Pandas' object has no attribute 'image_url'"
     ]
    }
   ],
   "source": [
    "api_key='acc_d2be34779581c77'\n",
    "api_secret='2648aa8fc9eb5305688d66089b6856f6'\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import requests, json\n",
    "df_imagga = pd.DataFrame(columns=[\"interior objects\", \"nature landscape\", \"beaches seaside\", \"events parties\", \"food drinks\",\n",
    "\"paintings art\", \"pets animals\", \"text visuals\", \"sunrises sunsets\", \"cars vehicles\",\n",
    "\"macro flowers\", \"streetview architecture\", \"people portraits\", 'tweet_id'])\n",
    "for row in df.itertuples():\n",
    "    tweet_id = row.tweet_id\n",
    "    if pd.isnull(row.image_url) or pd.isna(row.image_url):\n",
    "        scores = [0.0] * len(df_imagga.columns)\n",
    "\n",
    "        for n, col in enumerate(df_imagga.columns):\n",
    "            # Iterate through each category of the result\n",
    "            scores[n] = 0.0\n",
    "            # Store the list as a new row in the DataFrame\n",
    "            df_imagga.loc[tweet_id] = scores\n",
    "    else:\n",
    "        url = 'https://api.imagga.com/v2/categories/personal_photos/?image_url=' + row.image_url\n",
    "        request = requests.get(url, auth=(api_key, api_secret))\n",
    "        json_data = json.loads(request.text)\n",
    "\n",
    "        # Create a list of 0.0 scores to update as we get data for each category we want to score in our DataFrame\n",
    "        scores = [0.0] * len(df_imagga.columns)\n",
    "\n",
    "        # Find the associated column in the DataFrame\n",
    "        for n, col in enumerate(df_imagga.columns):\n",
    "            # Iterate through each category of the result\n",
    "            for category in json_data[\"result\"][\"categories\"]:\n",
    "                if col == category['name']['en']:\n",
    "                    # Store the score\n",
    "                    scores[n] = category['confidence']\n",
    "                    break  # No need to keep looping once we've found the score\n",
    "            # Store the list as a new row in the DataFrame\n",
    "            # scores.append(tweet_id)\n",
    "            df_imagga.loc[n] = scores\n",
    "            df_imagga = df_imagga.assign(tweet_id=tweet_id)\n",
    "# print(df_imagga.columns)\n",
    "# print(df.columns)\n",
    "# merge the two DataFrames\n",
    "# = pd.merge(df, df_imagga, left_on=df.tweet_id, right_on=df_imagga.index)\n",
    "df.join(df_imagga, how = 'left', lsuffix = '_left', rsuffix = '_right')\n",
    "# df_imagga.head()\n",
    "print(df.columns)\n",
    "print(df_imagga.columns)\n",
    "# df_imagga.index.name = 'tweet_id'\n",
    "df = df.merge(df_imagga, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Jackson\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key_0</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>sentiment_overall</th>\n",
       "      <th>sentiment_negative</th>\n",
       "      <th>sentiment_neutral</th>\n",
       "      <th>sentiment_positive</th>\n",
       "      <th>...</th>\n",
       "      <th>macro flowers</th>\n",
       "      <th>streetview architecture</th>\n",
       "      <th>people portraits</th>\n",
       "      <th>language_en</th>\n",
       "      <th>language_es</th>\n",
       "      <th>language_in</th>\n",
       "      <th>language_pt</th>\n",
       "      <th>source_Twitter Web App</th>\n",
       "      <th>source_Twitter for Android</th>\n",
       "      <th>source_Twitter for iPhone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.529656e-01</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.283954</td>\n",
       "      <td>0.511538</td>\n",
       "      <td>0.603306</td>\n",
       "      <td>0.212</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.998266</td>\n",
       "      <td>0.998266</td>\n",
       "      <td>9.886300e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.664719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.638017</td>\n",
       "      <td>0.438</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.997765</td>\n",
       "      <td>0.997765</td>\n",
       "      <td>6.627899e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.664719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.638017</td>\n",
       "      <td>0.438</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.997469</td>\n",
       "      <td>0.997469</td>\n",
       "      <td>9.955527e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.169231</td>\n",
       "      <td>0.348760</td>\n",
       "      <td>0.700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.994845</td>\n",
       "      <td>0.994845</td>\n",
       "      <td>9.205802e-01</td>\n",
       "      <td>0.003180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.555774</td>\n",
       "      <td>0.280769</td>\n",
       "      <td>0.690909</td>\n",
       "      <td>0.228</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>0.004469</td>\n",
       "      <td>0.004469</td>\n",
       "      <td>8.279884e-01</td>\n",
       "      <td>0.034207</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.664719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.664463</td>\n",
       "      <td>0.406</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>0.003925</td>\n",
       "      <td>0.003925</td>\n",
       "      <td>9.511404e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.699784</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.519008</td>\n",
       "      <td>0.582</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>0.003028</td>\n",
       "      <td>0.003028</td>\n",
       "      <td>8.908156e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003817</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.688413</td>\n",
       "      <td>0.288462</td>\n",
       "      <td>0.583471</td>\n",
       "      <td>0.354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>0.001840</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>9.951360e-11</td>\n",
       "      <td>0.024409</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.555774</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.662810</td>\n",
       "      <td>0.248</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.414766e-11</td>\n",
       "      <td>0.002063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.664719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.740496</td>\n",
       "      <td>0.314</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>530 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        key_0  tweet_id     author_id  retweet_count  like_count  \\\n",
       "0    1.000000  1.000000  8.529656e-01       0.001203    0.000000   \n",
       "1    0.998266  0.998266  9.886300e-01       0.000000    0.000000   \n",
       "2    0.997765  0.997765  6.627899e-01       0.000000    0.000000   \n",
       "3    0.997469  0.997469  9.955527e-01       0.000000    0.000000   \n",
       "4    0.994845  0.994845  9.205802e-01       0.003180    0.000000   \n",
       "..        ...       ...           ...            ...         ...   \n",
       "525  0.004469  0.004469  8.279884e-01       0.034207    0.000000   \n",
       "526  0.003925  0.003925  9.511404e-01       0.000000    0.000000   \n",
       "527  0.003028  0.003028  8.908156e-01       0.000000    0.003817   \n",
       "528  0.001840  0.001840  9.951360e-11       0.024409    0.000000   \n",
       "529  0.000000  0.000000  6.414766e-11       0.002063    0.000000   \n",
       "\n",
       "     possibly_sensitive  sentiment_overall  sentiment_negative  \\\n",
       "0                   0.0           0.283954            0.511538   \n",
       "1                   0.0           0.664719            0.000000   \n",
       "2                   0.0           0.664719            0.000000   \n",
       "3                   0.0           1.000000            0.169231   \n",
       "4                   0.0           0.555774            0.280769   \n",
       "..                  ...                ...                 ...   \n",
       "525                 0.0           0.664719            0.000000   \n",
       "526                 0.0           0.699784            0.000000   \n",
       "527                 0.0           0.688413            0.288462   \n",
       "528                 0.0           0.555774            0.307692   \n",
       "529                 0.0           0.664719            0.000000   \n",
       "\n",
       "     sentiment_neutral  sentiment_positive  ...  macro flowers  \\\n",
       "0             0.603306               0.212  ...            0.0   \n",
       "1             0.638017               0.438  ...            0.0   \n",
       "2             0.638017               0.438  ...            0.0   \n",
       "3             0.348760               0.700  ...            0.0   \n",
       "4             0.690909               0.228  ...            0.0   \n",
       "..                 ...                 ...  ...            ...   \n",
       "525           0.664463               0.406  ...            0.0   \n",
       "526           0.519008               0.582  ...            0.0   \n",
       "527           0.583471               0.354  ...            0.0   \n",
       "528           0.662810               0.248  ...            0.0   \n",
       "529           0.740496               0.314  ...            0.0   \n",
       "\n",
       "     streetview architecture  people portraits  language_en  language_es  \\\n",
       "0                        0.0               0.0          1.0          0.0   \n",
       "1                        0.0               0.0          0.0          1.0   \n",
       "2                        0.0               0.0          1.0          0.0   \n",
       "3                        0.0               0.0          1.0          0.0   \n",
       "4                        0.0               0.0          1.0          0.0   \n",
       "..                       ...               ...          ...          ...   \n",
       "525                      0.0               0.0          0.0          0.0   \n",
       "526                      0.0               0.0          1.0          0.0   \n",
       "527                      0.0               0.0          0.0          1.0   \n",
       "528                      0.0               0.0          1.0          0.0   \n",
       "529                      0.0               0.0          0.0          0.0   \n",
       "\n",
       "     language_in  language_pt  source_Twitter Web App  \\\n",
       "0            0.0          0.0                     0.0   \n",
       "1            0.0          0.0                     0.0   \n",
       "2            0.0          0.0                     0.0   \n",
       "3            0.0          0.0                     0.0   \n",
       "4            0.0          0.0                     1.0   \n",
       "..           ...          ...                     ...   \n",
       "525          0.0          1.0                     0.0   \n",
       "526          0.0          0.0                     0.0   \n",
       "527          0.0          0.0                     0.0   \n",
       "528          0.0          0.0                     1.0   \n",
       "529          1.0          0.0                     0.0   \n",
       "\n",
       "     source_Twitter for Android  source_Twitter for iPhone  \n",
       "0                           1.0                        0.0  \n",
       "1                           1.0                        0.0  \n",
       "2                           0.0                        1.0  \n",
       "3                           0.0                        1.0  \n",
       "4                           0.0                        0.0  \n",
       "..                          ...                        ...  \n",
       "525                         1.0                        0.0  \n",
       "526                         0.0                        0.0  \n",
       "527                         1.0                        0.0  \n",
       "528                         0.0                        0.0  \n",
       "529                         0.0                        1.0  \n",
       "\n",
       "[530 rows x 30 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./tweets.csv')\n",
    "\n",
    "# Data cleaning and Prep\n",
    "# Exclude columns it doesn't make sense to bin\n",
    "df = bin_groups(df, percent=.05, cols_to_exclude=['text', 'created_at', 'image_url'])\n",
    "# df = drop_columns_missing_data(df, cutoff=.50)\n",
    "# df = dummy_code_categorical_variables(df)\n",
    "# df = standardize_values(df)\n",
    "\n",
    "# Engineering\n",
    "df = calc_sentiment(df)\n",
    "df = image_classification(df=df, api_key='acc_d2be34779581c77', api_secret='2648aa8fc9eb5305688d66089b6856f6')\n",
    "\n",
    "# drop columns I'm finished analyzing\n",
    "df = df.drop(columns=['text', 'created_at', 'image_url'])\n",
    "\n",
    "# create dummy variables for categorical variables\n",
    "# df = dummy_code_categorical_variables(df)\n",
    "\n",
    "# impute KNN, also standardize values\n",
    "df = impute_KNN(df)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fs_select_linear(df, label=\"\"):\n",
    "  from sklearn.svm import LinearSVC\n",
    "  from sklearn.feature_selection import SelectFromModel\n",
    "  import pandas as pd\n",
    "\n",
    "  X = df.drop(label,axis=1)\n",
    "  y = df[label]\n",
    "\n",
    "  # As C increases, more features are kept\n",
    "  lsvc = LinearSVC(C=0.05, penalty=\"l1\", dual=False).fit(X, y)\n",
    "  sel = SelectFromModel(lsvc, prefit=True)\n",
    "  sel.transform(X)\n",
    "\n",
    "  columns = list(X.columns[sel.get_support()])\n",
    "  columns.append(label)\n",
    "  return df[columns]\n",
    "\n",
    "def fs_selectkbest(df, k=10, label=\"\"):\n",
    "  from sklearn.feature_selection import SelectKBest, r_regression\n",
    "  import pandas as pd\n",
    "\n",
    "  X = df.drop(columns=[label])\n",
    "  y = df[label]\n",
    "\n",
    "  # Select the top k features based on a given bivariate metric\n",
    "  sel = SelectKBest(r_regression, k=k)\n",
    "  sel.fit_transform(X, y)\n",
    "  \n",
    "  return df[sel.get_feature_names_out()].join(df[label])\n",
    "\n",
    "def fs_variance(df, label=\"\", p=0.8):\n",
    "  from sklearn.feature_selection import VarianceThreshold\n",
    "  import pandas as pd\n",
    "\n",
    "  if label != \"\":\n",
    "    X = df.drop(columns=[label])\n",
    "    \n",
    "  sel = VarianceThreshold(threshold=(p * (1 - p)))\n",
    "  sel.fit_transform(X)\n",
    "\n",
    "  # Add the label back in after removing poor features\n",
    "  return df[sel.get_feature_names_out()].join(df[label])\n",
    "\n",
    "# fs_linear = fs_select_linear(df, label=\"retweet_count\")\n",
    "# fs_variance = fs_variance(df, label=\"retweet_count\", p=.1)\n",
    "# fs_selectkbest = fs_selectkbest(df, 58, label=\"retweet_count\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling \n",
    "(same requirements of the modeling project)\n",
    "* Generate the best possible model for:\n",
    "    * Regression\n",
    "    * Classification\n",
    "    * Clustering\n",
    "* You choose which features to keep in the model model\n",
    "* There is no required level of fit metric. Your task is simply to get the best fit metrics possible--not achieve a certain value. All datasets are different and there is no way to compare them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algorithm_selection(df, cols_to_drop=[]):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn import model_selection\n",
    "    from sklearn import preprocessing\n",
    "    import sklearn.neural_network as nn\n",
    "    from sklearn.linear_model import RidgeCV, LassoCV\n",
    "    import sklearn.ensemble as se\n",
    "    import sklearn.tree as tree\n",
    "    from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n",
    "    from sklearn import gaussian_process\n",
    "    from sklearn import neighbors\n",
    "    from sklearn import svm\n",
    "    import sklearn.linear_model as lm\n",
    "    import pickle\n",
    "\n",
    "    df = df.select_dtypes(np.number)  # Remove categorical features first\n",
    "    y = df.like_count                    # Save the label first\n",
    "    # Remove the label from the feature list\n",
    "    X = df.drop(columns=cols_to_drop)\n",
    "\n",
    "    # Scale/normalize the features\n",
    "    X = pd.DataFrame(preprocessing.MinMaxScaler(\n",
    "    ).fit_transform(X), columns=X.columns)\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "        X, y, test_size=0.3, random_state=12345)\n",
    "\n",
    "    # Eyeball the data to make sure it looks right:\n",
    "    X_train\n",
    "\n",
    "    fit = {}  # Use this to store each of the fit metrics\n",
    "\n",
    "    # 1. LINEAR MODELS: assumes normal distribution, homoscedasticity, no multi-collinearity, independence, and no auto-correlation (some exceptions apply)\n",
    "\n",
    "    # 1.1. Ordinary Least Squares Multiple Linear Regression\n",
    "    model_ols = lm.LinearRegression()\n",
    "    model_ols.fit(X_train, y_train)\n",
    "    fit['OrdinaryLS R'] = model_ols.score(X_test, y_test)\n",
    "\n",
    "    # 1.2. Ridge Regression: more robust to multi-collinearity\n",
    "    # adjust this alpha parameter for better results (between 0 and 1)\n",
    "    model_rr = lm.Ridge(alpha=0.5)\n",
    "    model_rr.fit(X_train, y_train)\n",
    "    fit['Ridge R'] = model_rr.score(X_test, y_test)\n",
    "\n",
    "    # 1.3. Lasso Regression: better for sparse values like RetweetCount where most are zeros but a few have many retweets.\n",
    "    # adjust this alpha parameter for better results (between 0 and 1)\n",
    "    model_lr = lm.Lasso(alpha=0.1)\n",
    "    model_lr.fit(X_train, y_train)\n",
    "    fit['Lasso R'] = model_lr.score(X_test, y_test)\n",
    "\n",
    "    # 1.4. Least Angle Regression: good when the number of features is greater than the number of samples\n",
    "    # adjust this alpha parameter for better results (between 0 and 1)\n",
    "    model_llr = lm.LassoLars(alpha=0.1)\n",
    "    model_llr.fit(X_train, y_train)\n",
    "    fit['LARS Lasso R'] = model_llr.score(X_test, y_test)\n",
    "\n",
    "    # 1.5. Bayesian Regression: probability based; allows regularization parameters, automatically tuned to data\n",
    "    model_br = lm.BayesianRidge()\n",
    "    model_br.fit(X_train, y_train)\n",
    "    fit['Bayesian R'] = model_br.score(X_test, y_test)\n",
    "\n",
    "    # SUPPORT VECTOR MACHINES\n",
    "    # 1.9. SVM: this is the default SVM, parameters can be modified to make this more accurate\n",
    "    model_svm = svm.SVR()\n",
    "    model_svm.fit(X_train, y_train)\n",
    "    fit['SupportVM R'] = model_svm.score(X_test, y_test)\n",
    "\n",
    "    # 1.10. Linear SVM: Faster than SVM but only considers a linear model\n",
    "    model_lsvm = svm.LinearSVR()\n",
    "    model_lsvm.fit(X_train, y_train)\n",
    "    fit['Linear SVM R'] = model_lsvm.score(X_test, y_test)\n",
    "\n",
    "    # 1.11. NuSVM:\n",
    "    model_nusvm = svm.NuSVR()\n",
    "    model_nusvm.fit(X_train, y_train)\n",
    "    fit['NuSupportVM R'] = model_nusvm.score(X_test, y_test)\n",
    "\n",
    "    # STOCHASTIC GRADIENT DESCENT REGRESSION\n",
    "    # 1.12. SGDRegressor:\n",
    "    model_sgdr = lm.SGDRegressor()\n",
    "    model_sgdr.fit(X_train, y_train)\n",
    "    fit['SGradientD R'] = model_sgdr.score(X_test, y_test)\n",
    "\n",
    "    # KNN: NEAREST NEIGHBORS REGRESSION\n",
    "\n",
    "    # 1.13. KNeighborsRegressor:\n",
    "    model_knnr = neighbors.KNeighborsRegressor(5, 'uniform')\n",
    "    model_knnr.fit(X_train, y_train)\n",
    "    fit['KNNeighbors R'] = model_knnr.score(X_test, y_test)\n",
    "\n",
    "    # 1.14. KNeighborsRegressor:\n",
    "    model_knnrd = neighbors.KNeighborsRegressor(8, 'distance')\n",
    "    model_knnrd.fit(X_train, y_train)\n",
    "    fit['KNNeighborsD R'] = model_knnrd.score(X_test, y_test)\n",
    "\n",
    "    # GAUSSIAN PROCESS REGRESSION\n",
    "\n",
    "    # 1.15. GaussianProcessRegressor:\n",
    "    model_gpr = gaussian_process.GaussianProcessRegressor(\n",
    "        DotProduct() + WhiteKernel())\n",
    "    model_gpr.fit(X_train, y_train)\n",
    "    fit['GaussianP R'] = model_gpr.score(X_test, y_test)\n",
    "\n",
    "    # DECISION TREE MODELS: no assumptions about the data\n",
    "\n",
    "    # 1.16. Decision Tree Regression\n",
    "    model_dt = tree.DecisionTreeRegressor(random_state=12345)\n",
    "    model_dt.fit(X_train, y_train)\n",
    "    fit['Dec Tree R'] = model_dt.score(X_test, y_test)\n",
    "\n",
    "    # DECISION TREE-BASED ENSEMBLE MODELS: great for minimizing overfitting, these are based on averaging many unique sub-samples and combining algorithms\n",
    "    # 1.17. Decision Forrest\n",
    "    model_df = se.RandomForestRegressor(random_state=12345)\n",
    "    model_df.fit(X_train, y_train)\n",
    "    fit['Dec Forest R'] = model_df.score(X_test, y_test)\n",
    "\n",
    "    # 1.18. ExtraTreesRegressor\n",
    "    model_etr = se.ExtraTreesRegressor(random_state=12345)\n",
    "    model_etr.fit(X_train, y_train)\n",
    "    fit['Extra Trees R'] = model_etr.score(X_test, y_test)\n",
    "\n",
    "    # 1.19. AdaBoostRegressor\n",
    "    model_abr = se.AdaBoostRegressor(n_estimators=100, random_state=12345)\n",
    "    model_abr.fit(X_train, y_train)\n",
    "    fit['AdaBoost DT R'] = model_abr.score(X_test, y_test)\n",
    "\n",
    "    # 1.20. GradientBoostingRegressor\n",
    "    model_gbr = se.GradientBoostingRegressor(random_state=12345)\n",
    "    model_gbr.fit(X_train, y_train)\n",
    "    fit['Grad. Boost R'] = model_gbr.score(X_test, y_test)\n",
    "\n",
    "    # 1.22. VotingRegressor: will combine other algorithms into an average; kind of cool\n",
    "    model_vr = se.VotingRegressor(estimators=[('DT', model_dt), ('DF', model_df), (\n",
    "        'ETR', model_etr), ('ABR', model_abr), ('GBR', model_gbr)])\n",
    "    model_vr.fit(X_train, y_train)\n",
    "    fit['Voting R'] = model_vr.score(X_test, y_test)\n",
    "\n",
    "    # 1.23. StackingRegressor\n",
    "    estimators = [('ridge', RidgeCV()), ('lasso', LassoCV(\n",
    "        random_state=42)), ('svr', svm.SVR(C=1, gamma=1e-6))]\n",
    "    model_sr = se.StackingRegressor(\n",
    "        estimators=estimators, final_estimator=se.GradientBoostingRegressor(random_state=12345))\n",
    "    model_sr.fit(X_train, y_train)\n",
    "    fit['Stacking R'] = model_sr.score(X_test, y_test)\n",
    "\n",
    "    # NEURAL-NETWORK MODELS: Based on deep learning methods\n",
    "\n",
    "    # 1.24. MLPRegressor\n",
    "    # Turn max_iter way up or down to get a more accurate result\n",
    "    model_nn = nn.MLPRegressor(max_iter=1000, random_state=12345)\n",
    "    model_nn.fit(X_train, y_train)\n",
    "    fit['NeuralNet R'] = model_nn.score(X_test, y_test)\n",
    "\n",
    "    # Sort and print the dictionary by greatest R squared to least\n",
    "    r2s = sorted_list_by_value = sorted(fit, key=fit.__getitem__, reverse=True)\n",
    "    for r2 in r2s:\n",
    "        print(f'{r2}:\\t{fit[r2]}')\n",
    "\n",
    "    # Select the model with the highest R squared\n",
    "    print(f'Best model: {r2s[0]} (R2:{fit[r2s[0]]})')\n",
    "    model = fit[r2s[1]]\n",
    "    type(model)\n",
    "\n",
    "    # Save the model with the highest fit metric\n",
    "    pickle.dump(model, open('stored_model.sav', 'wb'))\n",
    "\n",
    "\n",
    "def clustering(df):\n",
    "    !pip install gower\n",
    "    import gower\n",
    "    from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "    distance_matrix = gower.gower_matrix(df)\n",
    "    agg = AgglomerativeClustering(\n",
    "        affinity='precomputed', linkage='average').fit(distance_matrix)\n",
    "\n",
    "    # make a cluster column\n",
    "    df_wcluster = df.copy()\n",
    "    df_wcluster['cluster'] = agg.labels_\n",
    "    df_wcluster.head()\n",
    "    return df_wcluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average R-squared:\t-82.46309669109394\n",
      "Requirement already satisfied: gower in c:\\users\\jackson\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (0.0.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\jackson\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from gower) (1.21.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\jackson\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from gower) (1.5.2)\n"
     ]
    }
   ],
   "source": [
    "df_modeling = df.copy()\n",
    "\n",
    "# algorithm_selection(df_modeling, cols_to_drop=['key_0', 'tweet_id', 'author_id', 'like_count', 'retweet_count'])\n",
    "\n",
    "df_MLR = fit_crossvalidate_mlr(df_modeling, 10, 'like_count', True)\n",
    "\n",
    "df_modeling = clustering(df_modeling)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automation\n",
    "* Dynamically select the best algorithm for your regression and classification models\n",
    "* Demonstrate through the feature importance metric which features should be included in the model. However, you don't need to set up an automated selection of features. You can manually decide which features to include\n",
    "* Dynamically save the best fitting model to a .sav file in the same folder as your .ipynb\n",
    "* Arrange your .ipynb file so that the \"Run all\" command will handle all steps above in order from data collection to .sav file"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f8ec8e2802e9ec64c1de1126b52a3a3eba2bbbc7f0465a520b33d3486dfa46c4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
